{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72349d9",
   "metadata": {},
   "source": [
    "# **ISSCC 2026 Code-a-Chip Challenge**\n",
    "# **LLMForge: Joint HW/SW Optimization of Edge-Scale Transformer Architecture**\n",
    "## Author: Xinting Jiang, Minxing Chu - Brown University \n",
    "\n",
    "xinting_jiang@brown.edu, ReaLLMASIC Lab\n",
    "\n",
    "Please feel free to reach out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47a9f2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook documents our end-to-end workflow for transformer hardware/software co-design for the ISSCC 2026 Code-a-Chip Challenge. The exploration is motivated by the need to deliver competitive language models on resource-constrained edge platforms, where accuracy gains must be balanced against strict latency, energy, and silicon budgets.\n",
    "\n",
    "We proceed in six stages:\n",
    "- formalize the GPT-2–style transformer search space and expose interactive controls for key architectural levers.\n",
    "- quantify the software-only configuration space before extending it to hardware-aware combinations.\n",
    "- map candidate transformer layers onto the custom accelerator while enforcing feasibility constraints from the systolic array and SRAM fabric;\n",
    "- sweep feasible HW/SW pairs to collect performance, power, and area metrics suitable for downstream analytics.\n",
    "- visualize Pareto trade-offs across latency, energy, and accuracy to surface promising design points.\n",
    "- motivate heterogeneous transformers and Infinite Attention as forward-looking avenues that further expand the co-design space.\n",
    "\n",
    "Github repo used: `https://github.com/ReaLLMASIC/ReaLLM-Forge.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075978a",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "### Installing Yosys, Openroad, sky130pdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f092524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "!pip install matplotlib pandas pyinstaller numpy\n",
    "!apt-get install -y ruby-full time build-essential\n",
    "!apt install -f libqt4-designer libqt4-xml libqt4-sql libqt4-network libqtcore4 libqtgui4\n",
    "!curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n",
    "conda_prefix_path = pathlib.Path('conda-env')\n",
    "site_package_path = conda_prefix_path / 'lib/python3.7/site-packages'\n",
    "sys.path.append(str(site_package_path.resolve()))\n",
    "CONDA_PREFIX = str(conda_prefix_path.resolve())\n",
    "PATH = os.environ['PATH']\n",
    "LD_LIBRARY_PATH = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "%env CONDA_PREFIX={CONDA_PREFIX}\n",
    "%env PATH={CONDA_PREFIX}/bin:{PATH}\n",
    "%env LD_LIBRARY_PATH={CONDA_PREFIX}/lib:{LD_LIBRARY_PATH}\n",
    "!bin/micromamba create --yes --prefix $CONDA_PREFIX\n",
    "!echo 'python ==3.7*' >> {CONDA_PREFIX}/conda-meta/pinned\n",
    "!bin/micromamba install --yes --prefix $CONDA_PREFIX \\\n",
    "                        --channel litex-hub \\\n",
    "                        --channel main \\\n",
    "                        open_pdks.sky130a \\\n",
    "                        openroad \\\n",
    "                        yosys\n",
    "!bin/micromamba install --yes --prefix $CONDA_PREFIX \\\n",
    "                        --channel conda-forge \\\n",
    "                        tcllib gdstk pyyaml click svgutils ngspice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f132e30",
   "metadata": {},
   "source": [
    "## Transformer Architecture Design Space\n",
    "\n",
    "Considering the gpt2-like decoder-only transformer structure, it would have the follow parameter config variations.\n",
    "\n",
    "![Transformer Architecture](images/transformer_sw.png)\n",
    "\n",
    "| Symbol | Parameter | Description | \n",
    "|:--:|:--|:--|\n",
    "| $\\text{n\\_embd}$ | Model dimension | Hidden size of layer $\\ell$ |\n",
    "| $\\text{n\\_head}$ | Attention heads | Number of parallel attention heads | \n",
    "| $\\text{mlp\\_ratio}$ | MLP expansion ratio | Expansion ratio in feedforward block | \n",
    "| $\\text{block\\_size}$ | Block size | Number of visible tokens per query | \n",
    "| $\\text{n\\_layers}$ | Layers | Number of Layer |\n",
    "\n",
    "Consider a homogeneous architecture (i.e. each layer shares the same setting), the search space could be configured in the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d361fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive search-space controls\n",
    "!pip install ipywidgets\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def show_config(max_n_layers,\n",
    "                n_embd_min, n_embd_max, n_embd_step,\n",
    "                mlp_min, mlp_max, mlp_step,\n",
    "                bs_min, bs_max, bs_step,\n",
    "                max_n_heads):\n",
    "    print(f\"Maximum number of layers: {max_n_layers}\")\n",
    "    print(f\"Embedding dimension range: min={n_embd_min}, max={n_embd_max}, step={n_embd_step}\")\n",
    "    print(f\"MLP ratio range: min={mlp_min}, max={mlp_max}, step={mlp_step}\")\n",
    "    print(f\"Block size range: min={bs_min}, max={bs_max}, step={bs_step}\")\n",
    "    print(f\"Maximum number of heads: {max_n_heads}\")\n",
    "\n",
    "    # Basic validation feedback\n",
    "    errs = []\n",
    "    if n_embd_step <= 0:\n",
    "        errs.append(\"Embedding step must be > 0\")\n",
    "    if n_embd_min > n_embd_max:\n",
    "        errs.append(\"Embedding min cannot be greater than max\")\n",
    "\n",
    "    if mlp_step <= 0:\n",
    "        errs.append(\"MLP ratio step must be > 0\")\n",
    "    if mlp_min > mlp_max:\n",
    "        errs.append(\"MLP ratio min cannot be greater than max\")\n",
    "\n",
    "    if bs_step <= 0:\n",
    "        errs.append(\"Block size step must be > 0\")\n",
    "    if bs_min > bs_max:\n",
    "        errs.append(\"Block size min cannot be greater than max\")\n",
    "\n",
    "    if errs:\n",
    "        print(\"\\nIssues:\")\n",
    "        for e in errs:\n",
    "            print(\" -\", e)\n",
    "\n",
    "\n",
    "# Wider labels so full names are visible\n",
    "label_style = {\"description_width\": \"initial\"}\n",
    "\n",
    "# Widgets with full names\n",
    "w_max_n_layers = widgets.IntSlider(\n",
    "    value=12, min=1, max=64, step=1,\n",
    "    description=\"Max n_layers\",\n",
    "    style=label_style,\n",
    ")\n",
    "\n",
    "# Embedding (d_model) range\n",
    "w_n_embd_min = widgets.IntText(\n",
    "    value=256, description=\"Embedding dimension (min)\",\n",
    "    style=label_style,\n",
    ")\n",
    "w_n_embd_max = widgets.IntText(\n",
    "    value=768, description=\"Embedding dimension (max)\",\n",
    "    style=label_style,\n",
    ")\n",
    "w_n_embd_step = widgets.IntText(\n",
    "    value=64, description=\"Embedding dimension (step)\",\n",
    "    style=label_style,\n",
    ")\n",
    "\n",
    "# MLP ratio range\n",
    "w_mlp_min = widgets.IntText(\n",
    "    value=1, description=\"MLP ratio (min)\",\n",
    "    style=label_style,\n",
    ")\n",
    "w_mlp_max = widgets.IntText(\n",
    "    value=8, description=\"MLP ratio (max)\",\n",
    "    style=label_style,\n",
    ")\n",
    "w_mlp_step = widgets.IntText(\n",
    "    value=1, description=\"MLP ratio (step)\",\n",
    "    style=label_style,\n",
    ")\n",
    "\n",
    "# Block size range\n",
    "w_bs_min = widgets.IntText(\n",
    "    value=256, description=\"Block size (min)\",\n",
    "    style=label_style,\n",
    ")\n",
    "w_bs_max = widgets.IntText(\n",
    "    value=2048, description=\"Block size (max)\",\n",
    "    style=label_style,\n",
    ")\n",
    "w_bs_step = widgets.IntText(\n",
    "    value=64, description=\"Block size (step)\",\n",
    "    style=label_style,\n",
    ")\n",
    "\n",
    "# Heads\n",
    "w_max_n_heads = widgets.IntSlider(\n",
    "    value=8, min=1, max=64, step=1,\n",
    "    description=\"Max n_heads\",\n",
    "    style=label_style,\n",
    ")\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    w_max_n_layers,\n",
    "    widgets.HBox([w_n_embd_min, w_n_embd_max, w_n_embd_step]),\n",
    "    widgets.HBox([w_mlp_min, w_mlp_max, w_mlp_step]),\n",
    "    widgets.HBox([w_bs_min, w_bs_max, w_bs_step]),\n",
    "    w_max_n_heads,\n",
    "])\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def _refresh_output(*_):\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        show_config(\n",
    "            w_max_n_layers.value,\n",
    "            w_n_embd_min.value, w_n_embd_max.value, w_n_embd_step.value,\n",
    "            w_mlp_min.value, w_mlp_max.value, w_mlp_step.value,\n",
    "            w_bs_min.value, w_bs_max.value, w_bs_step.value,\n",
    "            w_max_n_heads.value,\n",
    "        )\n",
    "\n",
    "\n",
    "for _w in [\n",
    "    w_max_n_layers,\n",
    "    w_n_embd_min, w_n_embd_max, w_n_embd_step,\n",
    "    w_mlp_min, w_mlp_max, w_mlp_step,\n",
    "    w_bs_min, w_bs_max, w_bs_step,\n",
    "    w_max_n_heads,\n",
    "]:\n",
    "    _w.observe(_refresh_output, names=\"value\")\n",
    "\n",
    "_refresh_output()\n",
    "\n",
    "# Apply button to save into variables\n",
    "apply_btn = widgets.Button(description=\"Apply to variables\", button_style=\"success\")\n",
    "status = widgets.HTML()\n",
    "\n",
    "\n",
    "def _on_apply_clicked(_):\n",
    "    global max_n_layers, n_embd, mlp_ratio, block_size, max_n_heads, config\n",
    "\n",
    "    # Validate before save\n",
    "    errs = []\n",
    "    if w_n_embd_step.value <= 0:\n",
    "        errs.append(\"Embedding step must be > 0\")\n",
    "    if w_n_embd_min.value > w_n_embd_max.value:\n",
    "        errs.append(\"Embedding min cannot be greater than max\")\n",
    "\n",
    "    if w_mlp_step.value <= 0:\n",
    "        errs.append(\"MLP ratio step must be > 0\")\n",
    "    if w_mlp_min.value > w_mlp_max.value:\n",
    "        errs.append(\"MLP ratio min cannot be greater than max\")\n",
    "\n",
    "    if w_bs_step.value <= 0:\n",
    "        errs.append(\"Block size step must be > 0\")\n",
    "    if w_bs_min.value > w_bs_max.value:\n",
    "        errs.append(\"Block size min cannot be greater than max\")\n",
    "\n",
    "    if errs:\n",
    "        status.value = \"<span style='color:red'>\" + \"; \".join(errs) + \"</span>\"\n",
    "        return\n",
    "\n",
    "    max_n_layers = w_max_n_layers.value\n",
    "    max_n_heads = w_max_n_heads.value\n",
    "    n_embd = {\n",
    "        \"min\": w_n_embd_min.value,\n",
    "        \"max\": w_n_embd_max.value,\n",
    "        \"step\": w_n_embd_step.value,\n",
    "    }\n",
    "    mlp_ratio = {\n",
    "        \"min\": w_mlp_min.value,\n",
    "        \"max\": w_mlp_max.value,\n",
    "        \"step\": w_mlp_step.value,\n",
    "    }\n",
    "    block_size = {\n",
    "        \"min\": w_bs_min.value,\n",
    "        \"max\": w_bs_max.value,\n",
    "        \"step\": w_bs_step.value,\n",
    "    }\n",
    "    config = dict(\n",
    "        max_n_layers=max_n_layers,\n",
    "        n_embd=n_embd,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "        block_size=block_size,\n",
    "        max_n_heads=max_n_heads,\n",
    "    )\n",
    "\n",
    "    status.value = (\n",
    "        \"<span style='color:green'>Saved: max_n_layers, n_embd {min,max,step}, mlp_ratio {min,max,step}, \"\n",
    "        \"block_size {min,max,step}, max_n_heads, and config</span>\"\n",
    "    )\n",
    "\n",
    "\n",
    "apply_btn.on_click(_on_apply_clicked)\n",
    "\n",
    "display(ui, out, apply_btn, status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a526093",
   "metadata": {},
   "source": [
    "## Check the Search Space Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: use the saved variables\n",
    "try:\n",
    "    print(\"max_n_layers:\", max_n_layers)\n",
    "    print(\"n_embd:\", n_embd)\n",
    "    print(\"max_n_heads:\", max_n_heads)\n",
    "    print(\"\\nConfig dict:\", config)\n",
    "except NameError:\n",
    "    print(\"Variables not set yet. Run the previous cell and click 'Apply to variables'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2de28e",
   "metadata": {},
   "source": [
    "## Inspect Search Space Size -- Homogeneous Architecture\n",
    "\n",
    "Inspect the size of the search space -- only considering variations on the software side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the search_space.py next to this notebook's folder and build a space from the UI variables\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the nsga_search directory (parent of this notebook) is on sys.path\n",
    "nb_dir = Path().resolve()              # .../nsga_search/notebooks\n",
    "src_dir = nb_dir.parent                # .../nsga_search\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "num_configs = 0 \n",
    "\n",
    "for n_embdin in range(n_embd[\"min\"], n_embd[\"max\"] + 1, n_embd[\"step\"]):\n",
    "    for n_heads in range(1, max_n_heads + 1):\n",
    "        if n_embdin % n_heads == 0:\n",
    "            num_configs += (mlp_ratio[\"max\"] - mlp_ratio[\"min\"] + 1) * ((block_size[\"max\"] - block_size[\"min\"]) // block_size[\"step\"] + 1)\n",
    "            \n",
    "num_configs *= max_n_layers  # arbitrary number of layers, each can be any of the configurations\n",
    "\n",
    "print(f\"Number of possible configurations in the search space: {num_configs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e2d58",
   "metadata": {},
   "source": [
    "### Mapping to Transformer Accelerator Hardware \n",
    "\n",
    "We designed a custom transformer accelerator which can run Transformer models defined above. \n",
    "\n",
    "![](images/transformer_hw.png)\n",
    "\n",
    "Figure (a) decribed the overall architecture of the systolic array based transformer accelerator, Figure (b) describes the structure with in a single core. \n",
    "\n",
    "We can explore this hardware search space with the parameters and constraints defined in the table below:\n",
    "\n",
    "| Parameter | Description | Range |\n",
    "|:--:|:--|:--|\n",
    "| $\\text{n\\_row}$ | Number of rows in the systolic array | keep the same number as the head size | \n",
    "| $\\text{n\\_col}$ | Number of columns in the systolic array | [1,64] | \n",
    "| $\\text{n\\_mac}$ | Number of mac units  | [4,8,16,32] | \n",
    "| $\\text{wmem\\_depth}$ | Block size | [128,256,512,768,1024,1536,2048,2560,3072,3584,4096] | \n",
    "| $\\text{kv\\_cache\\_depth}$ | Layers | [128,256,512,768,1024,1536,2048,2560,3072,3584,4096] |\n",
    "\n",
    "The computation for each head will be mapped onto one row of the systolic array by design. So we have n_row always equal to n_head.\n",
    "\n",
    "To adapt to the dataflow decribed in the figure, we have the following constraint for the n_mac and n_cols: \n",
    "\n",
    "<p align=\"center\"> ( n_embd / n_head / n_col ) % n_mac == 0 </p>\n",
    "<p align=\"center\"> block_size % n_mac == 0</p>\n",
    "\n",
    "![](images/n_cols_description.png)\n",
    "\n",
    "Based on the hardware search space description here, the hardware-software co-design space can be enlarged to the following size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_col = 64\n",
    "max_n_mac = 32\n",
    "\n",
    "num_hw_sw_configs = 0   \n",
    "num_hw_sw_raw_configs = 0\n",
    "\n",
    "def is_feasible(n_embd, n_heads, n_cols, n_macs, max_context_length) -> bool:\n",
    "    \"\"\"Check if the suggestion is feasible.\"\"\"\n",
    "\n",
    "    if n_embd % n_heads != 0:\n",
    "        # print(f\"n_embd {n_embd} is not divisible by n_heads {n_heads}. Reject suggestion.\")\n",
    "        return False\n",
    "    \n",
    "    head_dim = int(n_embd/n_heads)\n",
    "    if head_dim % n_cols != 0:\n",
    "        # print(f\"head_dim {head_dim} is not divisible by n_cols {n_cols}. Reject suggestion.\")\n",
    "        return False\n",
    "    \n",
    "    core_dim = int(head_dim/n_cols)\n",
    "    if core_dim % n_macs != 0:\n",
    "        # print(f\"core_dim {core_dim} is not divisible by mac_num {mac_num}. Reject suggestion.\")\n",
    "        return False\n",
    "    \n",
    "    if max_context_length % n_cols != 0:\n",
    "    #   print(f\"max_context_length {max_context_length} is not divisible by n_cols {n_cols}. Reject suggestion.\")\n",
    "      return False\n",
    "\n",
    "    return True\n",
    "\n",
    "for block_size_val in range(block_size[\"min\"], block_size[\"max\"] + 1, block_size[\"step\"]):\n",
    "    for n_embdin in range(n_embd[\"min\"], n_embd[\"max\"] + 1, n_embd[\"step\"]):\n",
    "        for n_heads in range(1, max_n_heads + 1):\n",
    "            if n_embdin % n_heads != 0:\n",
    "                continue\n",
    "            for n_col in range(1, max_n_col + 1):\n",
    "                for n_mac in range(1, max_n_mac + 1):\n",
    "                    num_hw_sw_raw_configs += (mlp_ratio[\"max\"] - mlp_ratio[\"min\"] + 1)\n",
    "                    if is_feasible(n_embdin, n_heads, n_col, n_mac, max_context_length=block_size_val):\n",
    "                        num_hw_sw_configs += (mlp_ratio[\"max\"] - mlp_ratio[\"min\"] + 1) \n",
    "\n",
    "print(f\"Number of hardware-software co-design configurations in the search space: {num_hw_sw_raw_configs}. After filtering infeasible ones: {num_hw_sw_configs}.\")\n",
    "print(f\"The HW/SW co-design search space is {num_hw_sw_configs/num_configs:.2f}x larger than the SW-only search space.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e1881",
   "metadata": {},
   "source": [
    "## HW Evaluation Methods\n",
    "\n",
    "Mapping SRAM configuration for each PE:\n",
    "\n",
    "Below is a compact summary of the SRAM blocks mapped per PE, with their width and depth expressions (using the search-space symbols):\n",
    "\n",
    "| Name | Width | Depth |\n",
    "|:--|:--:|:--:|\n",
    "| KV cache | `n_mac * 8` | `block_size * n_embd * 2 / n_row / n_col / n_mac` |\n",
    "| WMEM     | `n_mac * 8` | `(mlp_ratio * n_embd**2 + 2 * mr * n_embd) / n_row / n_col / n_mac` |\n",
    "\n",
    "For convenience, we round up the sram depth to the nearest value in this list: `[128,256,512,768,1024,1536,2048,2560,3072,3584,4096]`\n",
    "\n",
    "The way we did HW sweeping is by sweeping the configurations for each core(PE) in `sky130` pdk and then aggregate the hw metrics analytically. We use the hw metrics data after the synthesis step.\n",
    "\n",
    "The folling figure describes the flow to get hardware metrics(PPA)\n",
    "\n",
    "![](images/ppa_flow.png)\n",
    "\n",
    "Let's start with sweeping the search space for one core(PE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0e555",
   "metadata": {},
   "source": [
    "#### Run from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e59175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sys.path.append(path.join(path.dirname(__file__), '.'))\n",
    "from pyppa import PPARunner\n",
    "from pyppa.tools import Yosys, OpenROAD, Iverilog\n",
    "from platforms.sky130hd.config import SKY130HD_PLATFORM_CONFIG\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "# Initialize a PPA runner\n",
    "ppa_runner = PPARunner(\n",
    "\t# Design name can be anything\n",
    "\tdesign_name=\"core_top\",\n",
    "\t# Define the tools to be used here\n",
    "\ttools={\n",
    "\t\t'verilog_sim_tool': Iverilog(scripts_dir=path.join('scripts', 'iverilog')),\n",
    "\t\t'synth_tool': Yosys(scripts_dir=path.join('scripts', 'synth')),\n",
    "\t\t'ppa_tool': OpenROAD(scripts_dir=path.join('scripts', 'ppa'))\n",
    "\t},\n",
    "\t# The global flow configuration that applies to all jobs\n",
    "\tglobal_flow_config={\n",
    "\t\t# Source Verilog files.\n",
    "\t\t'VERILOG_FILES': [\n",
    "            path.join('verilog', 'sys_defs.svh'),\n",
    "            path.join('verilog', 'core/core_acc.v'),\n",
    "            path.join('verilog', 'core/core_buf.v'),\n",
    "            path.join('verilog', 'core/core_ctrl.v'),\n",
    "            path.join('verilog', 'core/core_mac.v'),\n",
    "            path.join('verilog', 'core/core_mem.v'),\n",
    "            path.join('verilog', 'core/core_quant.v'),\n",
    "            path.join('verilog', 'core/core_rc.v'),\n",
    "            path.join('verilog', 'core/core_top.v'),\n",
    "            path.join('verilog', 'util/pe.v'),\n",
    "            path.join('verilog', 'util/sram_sky130.v'),\n",
    "            path.join('verilog', 'util/align.v'),\n",
    "        \tpath.join('verilog', 'util/sky130_sram_stub.v')\n",
    "\t\t],\n",
    "\t\t# The constraint SDC file path.\n",
    "\t\t'SDC_FILE': path.join('verilog', 'constraint.sdc')\n",
    "\t}\n",
    ")\n",
    "\n",
    "# Set the platform configuration\n",
    "ppa_runner.set_platform(SKY130HD_PLATFORM_CONFIG)\n",
    "\n",
    "# Add a new sweep PPA job. This job sweeps a range of flow configurations and hyperparameters\n",
    "ppa_runner.add_job({\n",
    "\t# Name of the Verilog module to run the PPA job on\n",
    "\t'module_name': 'core_top',\n",
    "\t'mode': 'sweep',\n",
    "\t# This dictionary sets the flow configuration options for this job only. The options set here are appended to the global_flow_config options.\n",
    "\t# To use multiple sets of values (all of which will be swept), use a dictionary. See the option `ABC_AREA` below and `clk_period` in hyperparameters for more information.\n",
    "\t'flow_config': {\n",
    "\t\t# If this option is set to True, Verilgo simulations will be run using the verilog_sim_tool set above. In this example, IVerilog is used.\n",
    "\t\t'RUN_VERILOG_SIM': True,\n",
    "\t\t'RUN_VERILOG_SIM': True,\n",
    "\t\t# This sets the netlist used for running the Verilog simulations. In this case, the postsynthesis Verilog netlist will be used.\n",
    "\t\t'VERILOG_SIM_TYPE': 'postsynth',\n",
    "\t\t# A list of the required testbench files. The design files are automatically included and need not be added here.\n",
    "\t\t'VERILOG_TESTBENCH_FILES': [\n",
    "\t\t\tpath.join('verilog', 'core/core_top_tb.v')\n",
    "\t\t],\n",
    "\t\t# If this option is set to true, a VCD file dumped from the simulations will be used to get more accurate power estimates.\n",
    "\t\t'USE_STA_VCD': True,\n",
    "\t\t# The name of the VCD file dumped. By default it is set to `module_name.vcd`\n",
    "\t\t'VERILOG_VCD_NAME': 'core_top.vcd'\n",
    "\t\t,'ADDITIONAL_LIB_FILES': [\n",
    "            path.join('verilog', 'util/lib/sky130_sram_0kbytes_1rw_32x128_32.lib'),\n",
    "\t\t\tpath.join('verilog', 'util/lib/sky130_sram_2kbytes_1rw_32x512_32.lib')\n",
    "        ]\n",
    "\t\t,'ADDITIONAL_LEFS': [\n",
    "\t\t\tpath.join('verilog', 'util/lef/sky130_sram_0kbytes_1rw_32x128_32.lef'),\n",
    "\t\t\tpath.join('verilog', 'util/lef/sky130_sram_2kbytes_1rw_32x512_32.lef')\n",
    "        ]\n",
    "        ,'BLOCKS': [\n",
    "\t\t\t'sky130_sram_0kbytes_1rw_32x128_32',\n",
    "\t\t\t'sky130_sram_2kbytes_1rw_32x512_32'\n",
    "\t\t]\n",
    "\t\t,'ABC_MAX_FANOUT': {\n",
    "\t\t\t'start': 40,\n",
    "\t\t\t'end': 40,\n",
    "\t\t\t'step': 4\n",
    "\t\t},\n",
    "\t\t'ABC_MAP_EFFORT': {\n",
    "\t\t\t'values': [0.5]\n",
    "\t\t},\n",
    "\t\t'ABC_AREC_EFFORT': {\n",
    "            'values': [0.5]\n",
    "\t\t},\n",
    "\t\t'SYNTH_HIERARCHICAL': {\n",
    "\t\t\t'values': [False]\n",
    "\t\t}\n",
    "\t},\n",
    "\t# Hyperparameters are used defined parameters that can be inserted in the source files using the Mako templating syntax. See https://www.makotemplates.org/ for more information.\n",
    "\t# The simplest way is to write ${clk_period} in any source files (Verilog, Verilog testbench file, or constraint.sdc) to replace the value with the parameters set.\n",
    "\t# See the constraint.sdc file for example syntax usage.\n",
    "\t# The hyperparameters are swept along with the flow config, and all possible combinations of the options will be swept.\n",
    "\t'hyperparameters': {\n",
    "\t\t# The dictionary below defines a sweep for the `clk_period` hyperparameter. All values of clk_period, starting at `10` and going upto `100` will be swept with a step of 10. i.e., 10, 20, ..., 100.\n",
    "\t\t# This hyperparameter is used to set the clock period in the constraint.sdc and the verilog testbench.\n",
    "\t\t'clk_period': {\n",
    "\t\t\t'values': [3,4,5,6]\n",
    "\t\t},\n",
    "\t\t'mac_num': {\n",
    "\t\t\t'values': [4,8,16,32]\n",
    "\t\t},\n",
    "\t\t'wmem_depth': {\n",
    "\t\t\t'values': [128,256,512,768,1024,1536,2048,2560,3072,3584,4096]\n",
    "\t\t},\n",
    "\t\t'kv_cache_depth': {\n",
    "\t\t\t'values': [128,256,512,768,1024,1536,2048,2560,3072,3584,4096]\n",
    "\t\t}\n",
    "\t}\n",
    "})\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "ppa_runner.run_all_jobs()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "clk_period = []\n",
    "power = []\n",
    "area = []\n",
    "\n",
    "# Reading the PPA results in Python\n",
    "for job_run in ppa_runner.job_runs:\n",
    "\t# The `job_runs` variable contains the PPA results for each job\n",
    "\tfor ppa_run in job_run['ppa_runs']:\n",
    "\t\t# Each job run contains multiple \"PPA Runs\", each of which represents a particular configuration that was swept\n",
    "\t\tclk_period.append(ppa_run['ppa_stats']['sta']['clk']['clk_period'])\n",
    "\t\tpower.append(ppa_run['ppa_stats']['power_report']['total']['total_power'])\n",
    "\t\tarea.append(ppa_run['synth_stats']['module_area'])\n",
    "\t\t\n",
    "\t\tprint(\"\\n\")\n",
    "\t\tprint(f\"Results for run #{ppa_run['run_number']}:\")\n",
    "\t\t# print(f\"ABC max fanout: {ppa_run['flow_config']['ABC_MAX_FANOUT']}\")  # This is the value of the ABC_MAX_FANOUT option\n",
    "\t\t# print(f\"ABC map effort: {ppa_run['flow_config']['ABC_MAP_EFFORT']}\")  # This is the value of the ABC_MAP_EFFORT option\n",
    "\t\tprint(f\"wmem_depth: {ppa_run['hyperparameters']['wmem_depth']}, mac_num: {ppa_run['hyperparameters']['mac_num']}, clk_period: {ppa_run['hyperparameters']['clk_period']}\")\n",
    "\t\tprint(f\"PPA stats: {ppa_run['ppa_stats']['power_report']['total']['total_power']} W\")\n",
    "\t\tprint(f\"STA report: slack {ppa_run['ppa_stats']['sta']['clk']['clk_slack']} period {ppa_run['ppa_stats']['sta']['clk']['clk_period']} total {ppa_run['ppa_stats']['sta']['clk']['clk_period']+ppa_run['ppa_stats']['sta']['clk']['clk_slack']}\")\n",
    "\t\tprint(f\"Total cells={ppa_run['synth_stats']['num_cells']}, Area={ppa_run['synth_stats']['module_area']}, Seq/Comb cells = {ppa_run['ppa_stats']['num_sequential_cells']}/{ppa_run['ppa_stats']['num_combinational_cells']};\")\n",
    "\t\tprint(\"=========================================================\")\n",
    "\n",
    "print(clk_period)\n",
    "print(power)\n",
    "print(area)\n",
    "print(f\"Total time taken for PPA runs: {total_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d42e5",
   "metadata": {},
   "source": [
    "The completion time for this sweeping is over 12h. To save time, I put the extracted data in the ref_data folder `ref_data/core_top_sweep_extracted_data.csv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277918c",
   "metadata": {},
   "source": [
    "### Sweep SW-HW design points\n",
    "\n",
    "We also sweep the software design points with decoder-only architecture with size of 100M. We provided the data in the ref_data folder `ref_data/sw_sweep_data.csv`\n",
    "\n",
    "The following figure presents the sweeping process.\n",
    "\n",
    "![](images/sweep_flow.png)\n",
    "\n",
    "We will demo the sw and hw data matching process next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a59fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the sw swweep data and map it to sw_hw data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Load data again\n",
    "file_path = './ref_data/core_top_sweep_extracted_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Creating a nested dictionary-based database\n",
    "database = {}\n",
    "\n",
    "def is_feasible(n_embd, n_heads, n_cols, n_macs, max_context_length) -> bool:\n",
    "    \"\"\"Check if the suggestion is feasible.\"\"\"\n",
    "    if n_embd % n_heads != 0:\n",
    "        return False\n",
    "    head_dim = int(n_embd/n_heads)\n",
    "    if head_dim % n_cols != 0:\n",
    "        return False\n",
    "    core_dim = int(head_dim/n_cols)\n",
    "    if core_dim % n_macs != 0:\n",
    "        return False\n",
    "    if max_context_length % n_cols != 0:\n",
    "      return False\n",
    "    return True\n",
    "\n",
    "def get_cache_depth(n_model, n_heads, n_cols, max_context_length, n_macs) -> int:\n",
    "    \"\"\"Get the cache depth based on the suggestion.\"\"\"\n",
    "    raw_cache_depth = int(2 * n_model * max_context_length / n_macs / n_cols / n_heads)\n",
    "    if raw_cache_depth < 128:\n",
    "        return 128\n",
    "    elif raw_cache_depth < 256:\n",
    "        return 256\n",
    "    else:\n",
    "        return int(math.ceil(raw_cache_depth / 512) * 512)\n",
    "\n",
    "def get_wmem_depth(n_model, n_heads, n_cols, n_macs, ffn_ratio) -> int:\n",
    "    \"\"\"Get the wmem depth based on the suggestion.\"\"\"\n",
    "    raw_wmem_depth = int((4 + 0*ffn_ratio)*(n_model * n_model)/ n_heads / n_cols / n_macs)\n",
    "    if raw_wmem_depth < 32:\n",
    "        return 32\n",
    "    elif raw_wmem_depth < 64:\n",
    "        return 64\n",
    "    elif raw_wmem_depth < 128:\n",
    "        return 128\n",
    "    else :\n",
    "        return int(math.ceil(raw_wmem_depth / 512) * 512)\n",
    "    \n",
    "def get_TTFT(clk_period, n_model, mac_num, n_heads, n_cols, max_context_length, sequence_length=256 ,n_layers=1, ffn_ratio=4, softmax_choice='SOFTMAX', activation_choice='RELU'):\n",
    "    \"\"\"\n",
    "    Calculate the time to first token (TTFT) based on the design parameters.\n",
    "    \"\"\"\n",
    "    # coerce inputs to numeric types (some callers pass strings)\n",
    "    n_model = float(n_model)\n",
    "    mac_num = float(mac_num)\n",
    "    n_heads = float(n_heads)\n",
    "    n_cols = float(n_cols)\n",
    "    max_context_length = float(max_context_length)\n",
    "    sequence_length = float(sequence_length)\n",
    "    n_layers = float(n_layers)\n",
    "    ffn_ratio = float(ffn_ratio)\n",
    "    # Calculate the token delay for GEMM ops\n",
    "    TTFT = (4.0 * n_model * n_model + 2.0 * ffn_ratio * n_model * n_model) * 1e-9 * clk_period / gbus_width # load and store delay\n",
    "    # Number of cycles = Total MACs / (Number of MAC units)\n",
    "    num_loading_cycles = (4*n_model*n_model*sequence_length + 2*max_context_length*max_context_length*n_model + 2*ffn_ratio*n_model*n_model*sequence_length) / (n_heads*n_cols*mac_num)\n",
    "    TTFT += num_loading_cycles * clk_period * 1e-9 # seconds\n",
    "    # add 2 residual delay\n",
    "    TTFT += 2 * n_model * sequence_length * 1e-9 * clk_period # residual add delay\n",
    "    # add memory loading delay\n",
    "\n",
    "    # activation delay is negligible\n",
    "\n",
    "    TTFT /= 0.9 # assuming 90% efficiency\n",
    "\n",
    "    TTFT *= n_layers\n",
    "\n",
    "    return TTFT # convert to s\n",
    "\n",
    "def get_TPOT(clk_period, n_model, mac_num, n_heads, n_cols, max_context_length, sequence_length=256 ,n_layers=1, ffn_ratio=4, softmax_choice='SOFTMAX', activation_choice='RELU'):\n",
    "    \"\"\"\n",
    "    Calculate the time per output token (TPOT) based on the design parameters.\n",
    "    \"\"\"\n",
    "    # Calculate the token delay\n",
    "    # for GEMM ops\n",
    "\n",
    "    # coerce numeric inputs\n",
    "    n_model = float(n_model)\n",
    "    mac_num = float(mac_num)\n",
    "    n_heads = float(n_heads)\n",
    "    n_cols = float(n_cols)\n",
    "    max_context_length = float(max_context_length)\n",
    "    sequence_length = float(sequence_length)\n",
    "    n_layers = float(n_layers)\n",
    "    ffn_ratio = float(ffn_ratio)\n",
    "\n",
    "    TPOT = (4.0 * n_model * n_model + 2.0 * ffn_ratio * n_model * n_model) * 1e-9 * clk_period # load and store delay\n",
    "\n",
    "    # Number of cycles = Total MACs / (Number of MAC units)\n",
    "    num_loading_cycles = (4*n_model*n_model + 2*sequence_length*sequence_length*n_model + 2*ffn_ratio*n_model*n_model) / (n_heads*n_cols*mac_num)\n",
    "    TPOT += num_loading_cycles * clk_period * 1e-9 # seconds\n",
    "\n",
    "    # add 2 residual delay\n",
    "    TPOT += 2 * n_model * 1e-9 * clk_period # residual add delay\n",
    "\n",
    "    # add memory loading delay\n",
    "    TPOT /= 0.9 # assuming 90% efficiency\n",
    "    TPOT *= n_layers\n",
    "    return TPOT # convert to s\n",
    "\n",
    "# Populating the database with configuration as keys and relevant metrics as values\n",
    "for _, row in data.iterrows():\n",
    "    key = (row['MAC NUM'], row['Wmem Depth'], row['Cache Depth'], row['Clock Period (ns) Entered'])\n",
    "    database[key] = {\n",
    "        'power': row['Power (W)'],\n",
    "        'slack': row['Clock Slack (ns)'],\n",
    "        'clk_min_period': row['Clock_Min_Period'],\n",
    "        'area': row['Area (um^2)']\n",
    "        # Additional metrics can be added here as needed\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(\"./ref_data/sw_sweep_data.csv\")\n",
    "\n",
    "n_cols_range = np.arange(1, 99).tolist()\n",
    "mac_num = [4,8,16,32]\n",
    "clk_periods = [3,4,5,6]\n",
    "sequence_length = 256\n",
    "\n",
    "total_count = 0\n",
    "feasible_configs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    n_model = row['n_embd']\n",
    "    n_heads = row['n_head']\n",
    "    max_context_length = row['block_size']\n",
    "    val_loss = row['best_val_loss']\n",
    "    n_layers = row['n_layer']\n",
    "    ffn_ratio = row['mlp_expansion_factor']\n",
    "    for clk_period in clk_periods:\n",
    "        for n_cols in n_cols_range:\n",
    "            for n_macs in mac_num:\n",
    "                if is_feasible(n_model, n_heads, n_cols, n_macs, max_context_length):\n",
    "                    # Calculate wmem_depth and cache_depth\n",
    "                    wmem_depth = get_wmem_depth(n_model, n_heads, n_cols, n_macs, ffn_ratio)\n",
    "                    cache_depth = get_cache_depth(n_model, n_heads, n_cols, max_context_length, n_macs)\n",
    "\n",
    "                    # Retrieve core power and area from the database\n",
    "                    core_power = database.get((n_macs, wmem_depth, cache_depth, clk_period), {}).get('power', 'N/A') \n",
    "                    core_area = database.get((n_macs, wmem_depth, cache_depth, clk_period), {}).get('area', 'N/A')\n",
    "                    clk_min_period = database.get((n_macs, wmem_depth, cache_depth, clk_period), {}).get('clk_min_period', 'N/A')\n",
    "                    slack = database.get((n_macs, wmem_depth, cache_depth, clk_period), {}).get('slack', 'N/A')\n",
    "\n",
    "                    # calculate perplexity\n",
    "                    perplexity = math.exp(val_loss) if val_loss != 'N/A' else 'N/A'\n",
    "\n",
    "                    if core_power != 'N/A' and core_area != 'N/A' and clk_min_period != 'N/A':\n",
    "                        total_count += 1\n",
    "                        gbus_width = n_macs * 8\n",
    "                        ttft = get_TTFT(clk_min_period, n_model, n_macs, n_heads, n_cols, max_context_length, ffn_ratio=ffn_ratio, n_layers=n_layers, sequence_length=sequence_length)\n",
    "                        tpot = get_TPOT(clk_min_period, n_model, n_macs, n_heads, n_cols, max_context_length, ffn_ratio=ffn_ratio, n_layers=n_layers, sequence_length=sequence_length)\n",
    "                        throughput = 1/(tpot) if tpot != 0 else 'N/A'\n",
    "\n",
    "                        total_area = core_area * n_heads * n_cols\n",
    "                        total_power = core_power * n_heads * n_cols\n",
    "                        feasible_configs.append({\n",
    "                            'n_embd': n_model,\n",
    "                            'n_head': n_heads,\n",
    "                            'block_size': max_context_length,\n",
    "                            'n_layer': n_layers,\n",
    "                            'mlp_expansion_factor': ffn_ratio,\n",
    "                            'n_cols': n_cols,\n",
    "                            'MAC NUM': n_macs,\n",
    "                            'Wmem Depth': wmem_depth,\n",
    "                            'Cache Depth': cache_depth,\n",
    "                            'best_val_loss': val_loss,\n",
    "                            'perplexity': perplexity,\n",
    "                            'Power (W)': total_power,\n",
    "                            'Area (um^2)': total_area,\n",
    "                            'Clock Period (ns) Entered': clk_period,\n",
    "                            'Clock_Min_Period': clk_min_period,\n",
    "                            'F_max (MHz)': 1000/clk_min_period if clk_min_period != 0 else 'N/A',\n",
    "                            'Clock Slack (ns)': slack,\n",
    "                            'TTFT (ms)': ttft * 1000,\n",
    "                            'TPOT (ms)': tpot * 1000,\n",
    "                            'Throughput (tokens/s)': throughput,\n",
    "                            'Energy per token (mJ/token)': total_power / throughput * 1000,\n",
    "                            'EDP (mJ * ms)': (total_power/throughput/1e6) * ttft,\n",
    "                            'EADP (mJ * ms * um^2)': (total_power/throughput/1e6) * ttft * total_area\n",
    "                        })\n",
    "\n",
    "print(f\"Total number of feasible configurations: {total_count}\")\n",
    "feasible_df = pd.DataFrame(feasible_configs)\n",
    "\n",
    "# Save to CSV\n",
    "feasible_csv_path = './ref_data/sweep_data_sw_hw.csv'\n",
    "feasible_df.to_csv(feasible_csv_path, index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed63c46",
   "metadata": {},
   "source": [
    "scatter plots with the three main metrics: val_loss, TTFT, Energy per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23774d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reload the newest dataset\n",
    "file_path = \"./ref_data/sweep_data_sw_hw.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean column names\n",
    "df = df.rename(columns=lambda x: x.strip().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\"))\n",
    "\n",
    "# Normalize EDP and EADP for comparison\n",
    "df[\"EDP_norm\"] = (df[\"EDP_mJ_*_ms\"] - df[\"EDP_mJ_*_ms\"].min()) / (df[\"EDP_mJ_*_ms\"].max() - df[\"EDP_mJ_*_ms\"].min())\n",
    "df[\"EADP_norm\"] = (df[\"EADP_mJ_*_ms_*_um^2\"] - df[\"EADP_mJ_*_ms_*_um^2\"].min()) / (df[\"EADP_mJ_*_ms_*_um^2\"].max() - df[\"EADP_mJ_*_ms_*_um^2\"].min())\n",
    "\n",
    "# Function for Pareto frontier\n",
    "def pareto_frontier(df, x, y):\n",
    "    points = df[[x, y]].values\n",
    "    points = points[np.argsort(points[:, 0])]\n",
    "    frontier = []\n",
    "    min_y = np.inf\n",
    "    for px, py in points:\n",
    "        if py < min_y:\n",
    "            frontier.append((px, py))\n",
    "            min_y = py\n",
    "    return np.array(frontier)\n",
    "\n",
    "# Metrics to plot separately\n",
    "metrics = {\n",
    "    # \"Power_W\": \"Power (W)\",\n",
    "    # \"Area_um^2\": \"Area (um^2)\",\n",
    "    \"TTFT_ms\": \"TTFT (ms)\",\n",
    "    # \"TPOT_ms\": \"TPOT (ms)\",\n",
    "    \"Energy_per_token_mJ_token\": \"Energy per Token (mJ/token)\",\n",
    "    # \"EDP_norm\": \"Normalized EDP\",\n",
    "    # \"EADP_norm\": \"Normalized EADP\"\n",
    "}\n",
    "\n",
    "# Generate 5 separate figures\n",
    "for metric, label in metrics.items():\n",
    "    pareto = pareto_frontier(df, metric, \"best_val_loss\")\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(df[metric], df[\"best_val_loss\"], s=5, alpha=0.3, label=\"All Points\")\n",
    "    plt.plot(pareto[:,0], pareto[:,1], c=\"red\", linewidth=2, label=\"Pareto Frontier\")\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel(\"Best Val Loss\")\n",
    "    plt.title(f\"Pareto Frontier: {label} vs Validation Loss\")\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.savefig(f\"../plots/pareto/pareto_{metric}.png\", dpi=500)\n",
    "\n",
    "# plot best_val_loss as the color map on TTFT vs Energy per token scatter plot\n",
    "pareto = pareto_frontier(df, \"TTFT_ms\", \"Energy_per_token_mJ_token\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df[\"TTFT_ms\"], df[\"Energy_per_token_mJ_token\"], s=5, alpha=0.3, label=\"All Points\")\n",
    "plt.plot(pareto[:,0], pareto[:,1], c=\"red\", linewidth=2, label=\"Pareto Frontier\")\n",
    "plt.xlabel(\"TTFT (ms)\")\n",
    "plt.ylabel(\"Energy per Token (mJ/token)\")\n",
    "plt.title(\"Energy per Token vs TTFT\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61eb19a",
   "metadata": {},
   "source": [
    "## Beyond Traditional Transformer Architecture\n",
    "\n",
    "The above exploration is based on gpt2-like transformer architecture, while the modern edge scale language models have adopted architectural variation to boost model effiency.\n",
    "\n",
    "### From Homogeneous to Heterogeneous Architecture\n",
    "\n",
    "With heterogeneous architecture, each layer can have different parameters such as number of heads and mlp size. Now if we consider the paramter range we set before, we could have a much larger search space by allowing heterogeneous architecture. As we can see from the following calculation, the total number of feasible designs go up to astronomical figures.\n",
    "\n",
    "![](images/hetero.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5b2cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space ready. L_max = 12\n",
      "Number of possible configurations in the search space: 1.68e+48 1.34e+43x larger than the homogeneous space\n"
     ]
    }
   ],
   "source": [
    "# Build specs from interactive values and instantiate the search space\n",
    "try:\n",
    "    # Globals belong in globals_spec\n",
    "    gspec = {\n",
    "        \"d_model\": {\"type\": \"int\", \"low\": n_embd[\"min\"], \"high\": n_embd[\"max\"], \"step\": n_embd[\"step\"]},\n",
    "    }\n",
    "    # Per-layer fields go into layer_spec\n",
    "    lspec = {\n",
    "        \"n_heads\": {\"type\": \"int\", \"low\": 1, \"high\": max_n_heads, \"step\": 1},\n",
    "        \"block_size\": {\"type\": \"int\", \"low\": block_size[\"min\"], \"high\": block_size[\"max\"], \"step\": block_size[\"step\"]},\n",
    "        \"mlp_ratio\": {\"type\": \"int\", \"low\": mlp_ratio[\"min\"], \"high\": mlp_ratio[\"max\"], \"step\": mlp_ratio[\"step\"]},\n",
    "        \"attn_type\": {\"type\": \"cat\", \"choices\": [\"mha\"]},\n",
    "    }\n",
    "    print(\"Search space ready. L_max =\", max_n_layers)\n",
    "except NameError:\n",
    "    print(\"Variables not set yet. Run the interactive cell above and click 'Apply to variables'.\")\n",
    "    \n",
    "num_configs_het = 0 \n",
    "\n",
    "for n_embdin in range(n_embd[\"min\"], n_embd[\"max\"] + 1, n_embd[\"step\"]):\n",
    "    for n_heads in range(1, max_n_heads + 1):\n",
    "        if n_embdin % n_heads == 0:\n",
    "            num_configs_het += (mlp_ratio[\"max\"] - mlp_ratio[\"min\"] + 1) * ((block_size[\"max\"] - block_size[\"min\"]) // block_size[\"step\"] + 1)\n",
    "            \n",
    "num_configs_het = num_configs_het ** max_n_layers  # Each layer can be any of the configurations\n",
    "\n",
    "# use scientific notation if too large\n",
    "print(f\"Number of possible configurations in the search space: {num_configs_het:.2e} {num_configs_het // num_configs :.2e}x larger than the homogeneous space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d057bea",
   "metadata": {},
   "source": [
    "###  Introducing Infinite Attention\n",
    "\n",
    "Original MHA has this disadvantage that the n_head and head_dim are tight to the number of embeddings, this will restrict us from exploring possibilities of head pruning and different layer sizes。\n",
    "\n",
    "We introduce Infinite Attention, in which head_dim does not have to be n_embd/n_head and v_head_dim and qk_head_dim can be different, enabling us to explore a broader search space.\n",
    "\n",
    "![](images/infinite_attention.png)\n",
    "\n",
    "The figure above shows the structure of Infiite Attention.\n",
    "\n",
    "The searchable parameters are documented on the following table:\n",
    "\n",
    "#### Global Parameter Space\n",
    "\n",
    "| Parameter | Type | Range / Choices | Step |\n",
    "|:--|:--|:--|:--|\n",
    "| $\\text{n\\_embd}$ | int | fixed 768 | — |\n",
    "| $\\text{block\\_size}$ | int | fixed 512 | — |\n",
    "| $\\text{use\\_concat\\_heads}$ | categorical | {true, false} | — |\n",
    "\n",
    "#### Per-Layer Parameter Space\n",
    "\n",
    "| Parameter | Type | Range / Choices | Step |\n",
    "|:--|:--|:--|:--|\n",
    "| $\\text{n\\_h}$ | int | 1 – 16 | 1 |\n",
    "| $\\text{n\\_kv}$ | int | 1 – 16 | 1 |\n",
    "| $\\text{mlp\\_size}$ | int | 256 – 4096 | 256 |\n",
    "| $\\text{qk\\_dim}$ | int | 32 – 512 | 32 |\n",
    "| $\\text{v\\_dim}$ | int | 32 – 512 | 32 |\n",
    "| $\\text{n\\_cproj}$ | int | 1 – 4 | 1 |\n",
    "| $\\text{use\\_concat\\_head}$ | bool | True/False | - |\n",
    "| $\\text{attention\\_variant}$ | categorical | {infinite, identity} | - |\n",
    "\n",
    "**identity means skipping the attention module for this layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f30b81",
   "metadata": {},
   "source": [
    "## Multi-Objective Optimization of Transformer SW Architecture Enabled by NSGA-II Architecture\n",
    "\n",
    "As mentioned before, now the size of the entire search space has gone up to a astronomical figure. Sweeping through each feasible design point becomes unrealistic. We need a smarter algorithm to help us explore new design points efficiently. Here we decide to implement NSGA2, a multi-objective variant of the genetic algorithm for the optimization strategy. \n",
    "\n",
    "![](images/nsga_framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155063d",
   "metadata": {},
   "source": [
    "We run the algorithm over our Infinite Attention architecture based heterogeneous search space.\n",
    "\n",
    "![](images/exp_settings.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a805362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run this experiment, follow the readme in the nsga_experiments folder from reallmasic repo\n",
    "# the experiment setup can be configured\n",
    "# the experiment is exposed to be run with a group of gpu machines with ssh ips defined in a yaml file\n",
    "\n",
    "# clone the repo first\n",
    "!git clone https://github.com/ReaLLMASIC/ReaLLM-Forge.git\n",
    "\n",
    "# install the conda environment\n",
    "!cd ReaLLM-Forge && bash dev_env_setup_scripts/00-setup-conda.sh\n",
    "\n",
    "# install the requirements\n",
    "!pip install -r requirements_gpu.txt\n",
    "\n",
    "# fecth and tokenize the minipile dataset\n",
    "!cd data/minipile && bash get_dataset.sh && python prepare.py -t input.txt \n",
    "\n",
    "# run the nsga experiment with the generated search space config\n",
    "!cd ../../optimization_and_search/nsga_search  \n",
    "\n",
    "# follow the README in the nsga_experiments folder and run the experiment bash\n",
    "!bash run_from_scratch.bash\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b92b4",
   "metadata": {},
   "source": [
    "\n",
    "The completion time of this experiment on 8 h100 gpus is about 1.5 day. The results for this experiment is presented below.\n",
    "\n",
    "![](images/exp_results.png)\n",
    "\n",
    "Preliminary results show our models advancing toward the accuracy–size Pareto front, outperforming GPT-2 small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
