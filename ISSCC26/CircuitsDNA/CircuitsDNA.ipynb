{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1pJcY6eSMd5"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1142VbFt8kLRz7amDi9UQt5LaJvIcTaZt?usp=sharing\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RT4F8tQclTwl"
   },
   "source": [
    "# CircuitsDNA: Evolutionary Synthesis of Approximate Hardware acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzxIs9VeZ3Cj"
   },
   "source": [
    "```\n",
    "Submission to IEEE SSCS Open-Source Ecosystem “Code-a-Chip” Travel Grant Awards at ISSCC'26\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFISj7qnZ01A"
   },
   "source": [
    "|Name|Affiliation|\n",
    "|:--:|:----------:|\n",
    "|Ruichen Qi <br /> Email ID: ruichen_qi@brown.edu|Brown University|\n",
    "|Junyi Luo <br /> Email ID: junyi_luo@brown.edu|Brown University|\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfN6vNNNl9fY"
   },
   "source": [
    "## 1. Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAteluC1JJF-"
   },
   "source": [
    "### 1.1 Background\n",
    "\n",
    "According to Moore’s Law and Dennard scaling, continuous transistor miniaturization since 1974 has enabled exponential growth in device density—doubling with each generation—while maintaining higher clock speeds under constant power density. However, around 2007, the benefits of Dennard scaling began to fade due to leakage currents and power density constraints, and by 2012, scaling had largely halted.\n",
    "\n",
    "Modern computing systems now face severe power and thermal bottlenecks. As transistor density continues to rise, heat dissipation has become a fundamental limitation: chips can no longer operate all transistors simultaneously without exceeding safe thermal limits. This results in “dark silicon,” where only a portion of the available cores can remain active to avoid overheating. Elevated temperatures also degrade device reliability, accelerating wear-out mechanisms and shortening system lifetime.\n",
    "\n",
    "To address these challenges, approximate computing has emerged as a promising solution. By relaxing accuracy requirements in error-tolerant applications, approximate designs can substantially reduce power consumption and heat generation while maintaining acceptable output quality. This paradigm is especially suitable for neural network and large language model (LLM) accelerators, which are inherently resilient to small arithmetic errors. Minor inaccuracies in multiplications or accumulations typically have negligible impact on model accuracy, allowing designers to adopt approximate multipliers, adders, or reduced-precision datapaths for significant savings in power, area, and latency.\n",
    "\n",
    "In hardware accelerators such as systolic arrays, which are highly computing-intensive, multiply–accumulate (MAC) units dominate both area and power consumption. Fig.1 shows an hardware architecture of systolic-array based hardware architecture for Large Language Model (LLM). These regular and massively parallel structures in hardware accelerators perform billions of MAC operations per second, making them ideal candidates for approximate design. Replacing exact multipliers or accumulators with approximate counterparts can yield substantial overall energy and area reductions with minimal loss in computational accuracy.\n",
    "\n",
    "Moreover, fine-tuning techniques can further mitigate hardware-induced errors. By retraining or adapting model parameters on the approximate hardware, most of the lost accuracy can be recovered while retaining energy efficiency gains. This makes approximate computing particularly attractive for large-scale AI accelerators, where computational and memory demands are exceptionally high.\n",
    "\n",
    "### 1.2 Motivation\n",
    "\n",
    "Despite its promise, approximate computing still faces practical challenges in logic synthesis. Traditional design methods—such as manual simplification or heuristic gate pruning often rely on structural assumptions and struggle to scale for complex arithmetic blocks like multipliers. These methods typically yield locally optimized solutions and lack the flexibility to explore the vast combinational design space.\n",
    "\n",
    "To overcome these limitations, genetic algorithms offer an effective alternative. By evolving populations of candidate circuits through mutation, crossover, and selection, GAs can efficiently explore discrete, non-linear design spaces without requiring gradient information or explicit models. Their ability to support multi-objective optimization makes them ideal for balancing trade-offs among accuracy, power, area, and delay.\n",
    "\n",
    "However, most GA-based approximate synthesis approaches remain limited to small-scale demonstrations and rarely connect circuit-level optimization to system-level evaluation. This work bridges that gap by introducing an end-to-end GA-driven framework that synthesizes approximate computing circuits and evaluates their real-world impact on neural network tasks such as image classifications.\n",
    "\n",
    "### 1.3 Notebook Overview\n",
    "\n",
    "This notebook presents a genetic-algorithm–based framework for approximate logic synthesis and its application-level evaluation on deep learning models including CIFAR-100 (ResNet-18/ResNet-20).\n",
    "The framework is shown in Fig.2. The framework supports both random circuit generation and optimization from a seed netlist, providing a complete flow from synthesis to performance analysis.\n",
    "\n",
    "Workflow summary:\n",
    "\n",
    "1. An 8-bit signed multiplier is synthesized and verified using Yosys-ABC, iVerilog, and OpenSTA.\n",
    "2. The genetic algorithm performs approximate logic synthesis on the extracted netlist under various error and area constraints.\n",
    "3. The evolved approximate design is analyzed in OpenSTA to extract power, timing, and LUT information.\n",
    "4. Finally, the approximate multipliers are integrated into deep learning workloads (ResNet-18 and ResNet-20) to evaluate accuracy–efficiency trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"./images/systolic_array_hardware_architecture.png\"\n",
    "       alt=\"systolic_array_hardware_architecture\"\n",
    "       style=\"width:80%; max-width:1600px; border:1px solid #ccc;\">\n",
    "  <p style=\"font-style:italic; color:gray;\">\n",
    "    Figure 1: Systolic array hardware architecture.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"./images/workflow.png\"\n",
    "       alt=\"workflow\"\n",
    "       style=\"width:80%; max-width:1200px; border:1px solid #ccc;\">\n",
    "  <p style=\"font-style:italic; color:gray;\">\n",
    "    Figure 2: Workflow in this notebook.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4qdBSkHKfFp"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Environment Configuration\n",
    "\n",
    "### 2.1 Open-source Tools Installation and Verification\n",
    "It is required to install the open-source tools using the provided script. This is because we rely on the latest versions and newly introduced features, and OpenSTA is not available in the conda repositories. The script will automatically clone the corresponding GitHub repositories of these tools and complete the installation process.\n",
    "\n",
    "The latest versions of Yosys, iVerilog, and OpenSTA should be installed. There are many installation tutorials available online. You can either install them manually by following the instructions on their GitHub pages, or use the script we’ve provided:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ergg8MtZUKvi"
   },
   "outputs": [],
   "source": [
    "# === 2.1 Open-source Tools Installation and Verification ===\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "setup_sh = os.path.join(os.getcwd(), 'setup.sh')\n",
    "if os.path.exists(setup_sh):\n",
    "    print(f'Running {setup_sh}...')\n",
    "    subprocess.run(['bash', setup_sh], check=True)\n",
    "else:\n",
    "    print(f'No setup.sh found at {setup_sh}; skipping create_env')\n",
    "\n",
    "def which_tool(name, required=False):\n",
    "    p = shutil.which(name)\n",
    "    if p:\n",
    "        print(f'[OK] {name} found: {p}')\n",
    "        return True\n",
    "    else:\n",
    "        if required:\n",
    "            print(f'[ERR] {name} not found in PATH')\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            print(f'[WARN] {name} not found in PATH')\n",
    "            return False\n",
    "\n",
    "# Icarus Verilog, yosys and sta are required\n",
    "which_tool('iverilog', required=True)\n",
    "which_tool('yosys', required=False)\n",
    "which_tool('sta', required=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create Python Virtual Environment\n",
    "\n",
    "This step sets up an isolated Python environment to manage dependencies for the project. It ensures that all required packages are installed locally without interfering with system-wide Python libraries. All the dependencies are saved in requirements.txt. The Python virtual environment is mainly used for training deep learning models. Section 6 requires a GPU for model training. If your current device does not support GPU acceleration, please keep only ‘pandas’ in the requirements.txt file, comment out the other dependencies, and use only the evolutionary algorithm to generate approximate computing circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpQ-LwCtKcNg",
    "outputId": "fc9a93b9-7953-44a7-db61-aaa7229234a5"
   },
   "outputs": [],
   "source": [
    "# === 2.2 Create Python Virtual Environment ===\n",
    "\n",
    "!python3 -m venv venv\n",
    "!venv/bin/python -m pip install --upgrade pip setuptools wheel ipykernel\n",
    "!venv/bin/python -m pip install --upgrade-strategy eager -r requirements.txt\n",
    "# !venv/bin/python -m ipykernel install --user --name=venv --display-name \"Python (venv)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gedLMGoWcp2E"
   },
   "source": [
    "For the following steps, please switch the python kernal to the virtual environment we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rb6QWz2LOBu"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. 8-bit Signed Multiplier Synthesis and Verification\n",
    "\n",
    "In this demonstration, an 8-bit signed multiplier is synthesized and verified based on the GF180 technology as an example. All necessary files for synthesis and verification are prepared. Only six basic gate types — AND, NAND, OR, XOR, XNOR, and INV are used for synthesis, though the framework can be easily extended to include other logic gates or even larger building blocks such as full adders (FA), barrel shifters, multipliers, etc. Here, we use these six gate types solely as a representative example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O7R6XsXx9DR"
   },
   "source": [
    "### 3.1 Goldenbrick Generation, Behavioral Simulation & Verification\n",
    "\n",
    "The following step automates the full verification pipeline for the 8-bit signed multiplier design:\n",
    "\n",
    "3.11 Goldenbrick Generation\n",
    "\n",
    "The “goldenbrick” serves as a golden reference output, generated by a Python script (goldenbrick.py) that computes the expected results of the multiplier for all input combinations. It writes these results into goldenbrick.txt under code/synthesis/goldenbrick/.\n",
    "\n",
    "3.12 Behavioral Simulation\n",
    "\n",
    "The script then compiles and runs a behavioral simulation using Icarus Verilog (iverilog + vvp). It includes the GF180 standard cell models (gf180mcu_fd_sc_mcu9t5v0.v, primitives.v), the multiplier RTL (mult_8bits.sv), and its corresponding testbench (mult_8bits_tb.sv). During simulation, the testbench writes the computed results to rtl_sim_output.txt and dumps waveforms into rtl.vcd.\n",
    "\n",
    "3.13 Result Checking\n",
    "\n",
    "Finally, the simulation output is compared against the goldenbrick reference. If both files match exactly, it confirms that the multiplier’s behavioral model is functionally correct. Otherwise, it reports mismatches, indicating possible logic or testbench issues. This flow thus connects Python-based golden reference generation with Verilog simulation and verification, ensuring consistency between algorithmic and hardware-level implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnGPI5zaLM9j"
   },
   "outputs": [],
   "source": [
    "# === 3.1. Goldenbrick Generation, Behavioral Simulation & Verification ===\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths (mirror Makefile variables)\n",
    "GF_STD_ROOT = Path('src')\n",
    "STD_CELLS = [str(GF_STD_ROOT / 'gf180mcu_fd_sc_mcu9t5v0.v'), str(GF_STD_ROOT / 'primitives.v')]\n",
    "SIM_FILES = 'code/synthesis/verilog/modules/mult_8bits.sv'\n",
    "TOP_LEVEL_TESTBENCH = 'code/synthesis/verilog/testbench/mult_8bits_tb.sv'\n",
    "VSIM_DIR = Path('code/synthesis/vsim')\n",
    "RTL_VVP = str(VSIM_DIR / 'rtl.vvp')\n",
    "RTL_VCD = str(VSIM_DIR / 'rtl.vcd')\n",
    "RTL_TXT_OUT = str(VSIM_DIR / 'rtl_sim_output.txt')\n",
    "IVFLAGS = ['-g2012', '-I../verilog/modules']\n",
    "\n",
    "# -- 3.11 Goldenbrick Generation: run python script and save to file --\n",
    "gb_path = Path('code/synthesis/goldenbrick/goldenbrick.txt')\n",
    "gb_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "print('Generating goldenbrick...')\n",
    "with open(gb_path, 'w') as f:\n",
    "    subprocess.run([sys.executable, 'code/synthesis/goldenbrick/goldenbrick.py', '--width', '8', '--signed'], stdout=f, check=True)\n",
    "print(f'Goldenbrick saved to {gb_path}')\n",
    "\n",
    "# -- 3.12 Behavior Simulation: run iverilog and vvp --\n",
    "VSIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "iverilog_cmd = ['iverilog'] + IVFLAGS + ['-o', RTL_VVP] + STD_CELLS + [SIM_FILES, TOP_LEVEL_TESTBENCH, '-D', 'FUNCTIONAL', '-D', f'VSIM_OUT=\"{RTL_TXT_OUT}\"', '-D', f'VCD_FILE=\"{RTL_VCD}\"']\n",
    "print('Compiling behavioral simulation with:', ' '.join(iverilog_cmd))\n",
    "subprocess.run(iverilog_cmd, check=True)\n",
    "subprocess.run(['vvp', RTL_VVP], check=True)\n",
    "print('Behavioral simulation finished; output written to', RTL_TXT_OUT)\n",
    "\n",
    "# -- 3.13 Result Checking: compare outputs --\n",
    "gold_file = gb_path\n",
    "sim_out = Path(RTL_TXT_OUT)\n",
    "if gold_file.exists() and sim_out.exists():\n",
    "    import filecmp\n",
    "    if filecmp.cmp(str(gold_file), str(sim_out), shallow=False):\n",
    "        print('Outputs match! Behavioral simulation passed!')\n",
    "    else:\n",
    "        print('Outputs differ!')\n",
    "else:\n",
    "    print('Golden or simulation output missing; cannot compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logic Synthesis, Gate-Level Simulation & Static Timing Analysis\n",
    "\n",
    "The following step performs the post-synthesis verification flow for the 8-bit signed multiplier design.\n",
    "\n",
    "3.21 Logic Synthesis\n",
    "\n",
    "This step invokes Yosys to synthesize the RTL design (mult_8bits.sv) into a gate-level netlist using the GF180MCU standard-cell library (gf180mcu_fd_sc_mcu9t5v0__tt_025C_3v30.lib).\n",
    "The resulting netlist (mult_8bits.syn.v) is saved under code/synthesis/syn/.\n",
    "This process transforms the high-level behavioral description into a technology-mapped implementation ready for physical analysis.\n",
    "\n",
    "3.22 Gate-Level Simulation\n",
    "\n",
    "After synthesis, the script runs a gate-level simulation using Icarus Verilog (iverilog + vvp).\n",
    "It uses the same testbench as the behavioral simulation, along with the synthesized netlist and GF180 standard-cell models.\n",
    "Simulation outputs are written to gl_sim_output.txt, and signal waveforms are dumped into gl.vcd for inspection.\n",
    "\n",
    "3.23 Output Checking\n",
    "\n",
    "The gate-level simulation results are compared against the goldenbrick reference.\n",
    "If both outputs match, it confirms that the synthesis preserved the intended functionality.\n",
    "Any mismatch indicates possible synthesis or simulation discrepancies.\n",
    "\n",
    "3.24 Static Timing Analysis (STA)\n",
    "\n",
    "If OpenSTA is available, the script runs sta.tcl to perform static timing analysis using the same GF180 library.\n",
    "This step reports the critical path delay, setup/hold time, and timing slack information, stored in code/synthesis/sta/sta.report.\n",
    "It verifies that the synthesized circuit meets timing requirements under typical operating conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.2. Logic Synthesis, Gate-Level Simulation & Static Timing Analysis ===\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths / vars\n",
    "GF_LIB = str(Path('src') / 'gf180mcu_fd_sc_mcu9t5v0__tt_025C_3v30.lib')\n",
    "SYN_FILES = Path('code/synthesis/verilog/modules/mult_8bits.sv')\n",
    "SYN_NETLIST = 'code/synthesis/syn/mult_8bits.syn.v'\n",
    "STD_CELLS = [str(Path('src') / 'gf180mcu_fd_sc_mcu9t5v0.v'), str(Path('src') / 'primitives.v')]\n",
    "TOP_LEVEL_TESTBENCH = 'code/synthesis/verilog/testbench/mult_8bits_tb.sv'\n",
    "VSIM_DIR = Path('code/synthesis/vsim')\n",
    "GL_VVP = str(VSIM_DIR / 'gl.vvp')\n",
    "GL_VCD = str(VSIM_DIR / 'gl.vcd')\n",
    "\n",
    "# -- 3.21 Logic Synthesis: call yosys in code/synthesis --\n",
    "print('Running synthesis (yosys)...')\n",
    "env = os.environ.copy()\n",
    "\n",
    "# Make GF_LIB absolute so it can be found regardless of the working directory used for yosys\n",
    "GF_LIB = str((Path.cwd() / 'src' / 'gf180mcu_fd_sc_mcu9t5v0__tt_025C_3v30.lib').resolve())\n",
    "env['LIB_SYN'] = GF_LIB\n",
    "try:\n",
    "    subprocess.run(['yosys', '-ql', 'syn/run_synth.log', '-c', 'syn/run_synth.tcl'], cwd='code/synthesis', check=True, env=env)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print('Yosys failed:')\n",
    "    print(e)\n",
    "    # Re-raise so the notebook cell still fails visibly after printing helpful info\n",
    "    raise\n",
    "if not SYN_FILES.exists():\n",
    "    raise SystemExit(f'[ERR] Synthesis did not produce {SYN_FILES}')\n",
    "print('[SYN] Done. Log: code/synthesis/syn/run_synth.log')\n",
    "\n",
    "# -- 3.22 Gate-level simulation --\n",
    "VSIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "iverilog_cmd = ['iverilog', '-g2012', '-I../verilog/modules', '-o', GL_VVP] + STD_CELLS + [SYN_NETLIST, TOP_LEVEL_TESTBENCH, '-D', f'VSIM_OUT=\"{VSIM_DIR}/gl_sim_output.txt\"', '-D', f'VCD_FILE=\"{GL_VCD}\"']\n",
    "print('Compiling gate-level simulation with:', ' '.join(iverilog_cmd))\n",
    "subprocess.run(iverilog_cmd, check=True)\n",
    "subprocess.run(['vvp', GL_VVP], check=True)\n",
    "\n",
    "# -- 3.23 Output Checking: compare outputs --\n",
    "gold = Path('code/synthesis/goldenbrick/goldenbrick.txt')\n",
    "gl_out = Path(str(VSIM_DIR / 'gl_sim_output.txt'))\n",
    "if gold.exists() and gl_out.exists():\n",
    "    import filecmp\n",
    "    if filecmp.cmp(str(gold), str(gl_out), shallow=False):\n",
    "        print('Outputs match! Gate-level netlist simulation passed!')\n",
    "    else:\n",
    "        print('Outputs differ!')\n",
    "else:\n",
    "    print('Golden or gate-level output missing; cannot compare')\n",
    "\n",
    "# -- 3.24 Static Timing Analysis: run OpenSTA if available --\n",
    "sta_bin = shutil.which('sta')\n",
    "if sta_bin:\n",
    "    Path('code/synthesis/sta').mkdir(parents=True, exist_ok=True)\n",
    "    env = os.environ.copy(); env['LIB_SYN'] = GF_LIB\n",
    "    with open('code/synthesis/sta/sta.report', 'w') as fout:\n",
    "        subprocess.run([sta_bin, 'code/synthesis/sta/sta.tcl'], check=True, stdout=fout, env=env)\n",
    "    print('[STA] Done. Reports in code/synthesis/sta')\n",
    "else:\n",
    "    print('[WARN] OpenSTA (sta) not found; skipping STA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_K1JaNfLmkV"
   },
   "source": [
    "---\n",
    "## 4. Approximate Multiplier Synthesis from the Synthesized Netlist via Genetic Algorithm\n",
    "\n",
    "In this section, we use a genetic algorithm (GA) to evolve approximate 8-bit signed multipliers. Genetic algorithms struggle with large circuits due to the exponentially growing search space, high computational cost, and poor scalability. Therefore, our GA starts from the seed circuit, which is loaded from the synthesized Verilog netlist in previous step.\n",
    "\n",
    "During each generation, the algorithm evaluates all candidate circuits by simulating their outputs over all input patterns. The fitness function jointly considers circuit accuracy and area efficiency — penalizing individuals with large worst-case errors (WCE > ε_th) while favoring smaller transistor counts. Multiple error metrics such as NMED, ER, WCE, MRE, and sMAPE are recorded for analysis.\n",
    "\n",
    "New individuals are created through mutation operators, including:\n",
    "1. Add / delete node: randomly insert or remove a gate;\n",
    "2. Change gate type: switch to another logic primitive;\n",
    "3. Rewire inputs or outputs: alter signal connectivity;\n",
    "4. Merge equivalent nodes: remove redundant subcircuits.\n",
    "\n",
    "By iteratively applying mutation, pruning inactive nodes, and selecting the best individuals, the GA searches the discrete, irregular design space to obtain compact approximate multipliers with bounded output error. This process enables automatic approximate logic synthesis directly at the gate level, providing a flexible framework for exploring accuracy–area trade-offs under the GF180 technology library.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./images/GA_pseudo_code.png\"\n",
    "       alt=\"GA_pseudo_code\"\n",
    "       style=\"width:80%; max-width:800px; border:1px solid #ccc;\">\n",
    "  <br>\n",
    "  <em>Figure 3: Pseudo code for the implemented genetic algorithm.</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/GA_pseudo_code_explanation.png\"\n",
    "       alt=\"GA_pseudo_code_explanation\"\n",
    "       style=\"width:80%; max-width:800px; border:1px solid #ccc;\">\n",
    "  <br>\n",
    "  <em>Figure 4: List of symbols and their definitions</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTQ4fcrMDQxO"
   },
   "source": [
    "### 4.1 Genetic Algorithm Compilation\n",
    "\n",
    "The following step compiles the C++ implementation of the genetic algorithm (GA) used for evolving approximate circuit designs. It compiles all necessary source files and produces the executable. This executable serves as the core evolution engine, responsible for generating, mutating, and evaluating circuit candidates during the genetic search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBfr9hpuLg16"
   },
   "outputs": [],
   "source": [
    "# === 4.1. Genetic Algorithm Compilation ===\n",
    "\n",
    "import subprocess\n",
    "print('Compiling genetic algorithm C++ sources...')\n",
    "subprocess.run(['g++', '-std=c++17', '-O3', '-Wall', '-Wextra', '-I.', 'file_io.cpp', 'gate.cpp', 'problem.cpp', 'utils.cpp', 'main.cpp', '-o', 'main_exec'], cwd='code/genetic_algorithm', check=True)\n",
    "print('Compilation finished. Executable: code/genetic_algorithm/main_exec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5tb4EUFSix2"
   },
   "source": [
    "### 4.2 Genetic Algorithm Execution\n",
    "\n",
    "The following step launches the genetic algorithm (GA) engine to evolve approximate multiplier designs based on the synthesized 8-bit netlist.\n",
    "\n",
    "4.21 Execution Setup\n",
    "\n",
    "The script first ensures that the compiled executable (main_exec) exists under code/genetic_algorithm/.\n",
    "If missing, it automatically recompiles the source files using g++ with optimization and warning flags.\n",
    "All paths are resolved relative to the repository root to maintain consistency with internal file references.\n",
    "\n",
    "4.22 Runtime Configuration\n",
    "\n",
    "The GA is executed with explicit runtime arguments, overriding the defaults defined in main.cpp.\n",
    "These parameters include:\n",
    "- Maximum number of generations: 100,000\n",
    "- Population size: 100\n",
    "- Top-k (elite) ratio: 2% of the population\n",
    "- Mutation probabilities: [0.10, 0.10, 0.10, 0.40, 0.15, 0.15] for different mutation operators\n",
    "- Error exploration schedule: eps_start=0.0, eps_end=0.1, eps_tau=2000.0\n",
    "- Seed: 2025\n",
    "\n",
    "The script runs the GA on the synthesized multiplier netlist (mult_8bits.syn.v) and writes all logs and evolved results into code/genetic_algorithm/output/.\n",
    "\n",
    "4.23 Evolution Process\n",
    "\n",
    "During execution, the GA iteratively generates, mutates, and evaluates circuit candidates to minimize area and error under given constraints.\n",
    "All key metrics, including fitness scores, generation progress, and best solutions, are saved in evolution_metrics_signed8x8.txt. This step effectively performs automated approximate circuit synthesis, bridging the synthesis flow and the evolutionary optimization stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDmtJXr-Lshl"
   },
   "outputs": [],
   "source": [
    "# === 4.2. Genetic Algorithm Execution ===\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# -- 4.21 Execution Setup --\n",
    "exe_rel = os.path.join('code', 'genetic_algorithm', 'main_exec')\n",
    "exe = os.path.abspath(exe_rel)\n",
    "\n",
    "# If missing, attempt to compile in the GA source directory\n",
    "if not os.path.exists(exe):\n",
    "    print(f'Executable not found at {exe}; attempting to compile in code/genetic_algorithm...')\n",
    "    try:\n",
    "        subprocess.run(['g++', '-std=c++17', '-O3', '-Wall', '-Wextra', '-I.', 'file_io.cpp', 'gate.cpp', 'problem.cpp', 'utils.cpp', 'main.cpp', '-o', 'main_exec'], cwd='code/genetic_algorithm', check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print('Compilation failed:')\n",
    "        print(e)\n",
    "        raise SystemExit('Could not compile GA executable; please check build errors.')\n",
    "    if not os.path.exists(exe):\n",
    "        raise SystemExit('Executable still missing after compilation attempt.')\n",
    "    \n",
    "# Run from repository root (top-level) so paths are consistent\n",
    "candidate_cwd = os.getcwd()\n",
    "print('Running GA executable from', candidate_cwd)\n",
    "\n",
    "# Determine netlist and output directory to pass as runtime args\n",
    "netlist_path = str(Path(candidate_cwd) / 'code' / 'synthesis' / 'syn' / 'mult_8bits.syn.v')\n",
    "output_dir = str(Path(candidate_cwd) / 'code' / 'genetic_algorithm' / 'output')\n",
    "logfile = str(Path(output_dir) / 'evolution_metrics_signed8x8.txt')\n",
    "\n",
    "# -- 4.22 Runtime Configuration --\n",
    "generations = '100000'\n",
    "pop_size = '100'\n",
    "top_k_ratio = '0.02'\n",
    "eps_start = '0.0'\n",
    "eps_end = '0.1'\n",
    "eps_tau = '2000.0'\n",
    "seed = '2025'\n",
    "\n",
    "# 1 = true, 0 = false for these flags as parsed by main.cpp\n",
    "load_from_file = '1'\n",
    "signed_mult = '1'\n",
    "\n",
    "# Mutation weights as a comma-separated string matching main.cpp parsing expectations\n",
    "mut_weights = '0.10,0.10,0.10,0.40,0.15,0.15'   #Add, delete, change_gate, merge_equiv, rewire_in, rewire_out\n",
    "cmd = [\n",
    "    exe,\n",
    "    '--netlist-file', netlist_path,\n",
    "    '--output-dir', output_dir,\n",
    "    '--logfile', logfile,\n",
    "    '--generations', generations,\n",
    "    '--pop-size', pop_size,\n",
    "    '--top-k-ratio', top_k_ratio,\n",
    "    '--eps-start', eps_start,\n",
    "    '--eps-end', eps_end,\n",
    "    '--eps-tau', eps_tau,\n",
    "    '--seed', seed,\n",
    "    '--load-from-file', load_from_file,\n",
    "    '--signed-mult', signed_mult,\n",
    "    '--mut-weights', mut_weights\n",
    "]\n",
    "print('Command:', ' '.join(cmd))\n",
    "\n",
    "# -- 4.23 Evolution Process --\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    proc = subprocess.run(cmd, cwd=candidate_cwd, check=True, capture_output=True, text=True)\n",
    "    if proc.stdout:\n",
    "        print('--- stdout ---')\n",
    "        print(proc.stdout)\n",
    "    if proc.stderr:\n",
    "        print('--- stderr ---')\n",
    "        print(proc.stderr)\n",
    "    print('GA run finished')\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print('GA executable failed with returncode', e.returncode)\n",
    "    if e.stdout:\n",
    "        print('--- stdout ---')\n",
    "        print(e.stdout)\n",
    "    if e.stderr:\n",
    "        print('--- stderr ---')\n",
    "        print(e.stderr)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Result Evaluation\n",
    "\n",
    "Here we use the evolution with an relative worst case error threshold of 0.5% as an example. The algorithm is very lightweight and can run on a regular laptop. All the experiments in this section were run for 10 hours on an AMD Ryzen 9 7845HX laptop with 32GB memory.\n",
    "\n",
    "4.31 Evolution process of an signed 8-bit approximate multiplier. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/evolution_process_epsEnd_0.005.png\"\n",
    "       alt=\"evolution_process_epsEnd_0.005\"\n",
    "       style=\"width:80%; max-width:800px; border:1px solid #ccc;\">\n",
    "  <br>\n",
    "  <em>Figure 5: Evolution process with relative worst-case error threshold of 0.5%.</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.32 Equivalent transistor count and error metrics under various relative worst-case error thresholds\n",
    "\n",
    "A grid search over the relative worst-case error threshold was performed, and the results are shown below. With only a 0.5% relative worst-case error, the equivalent transistor count was reduced to 76.5% of the original size. All the evolved circuit netlists as well as look-up tables are provided in src/ as a reference.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/equivalent_transistor_count_vs_epsEnd.png\"\n",
    "       alt=\"equivalent_transistor_count_vs_epsEnd\"\n",
    "       style=\"width:80%; max-width:800px; border:1px solid #ccc;\">\n",
    "  <br>\n",
    "  <em>Figure 6: Equivalent transistor count versus relative worst-case error threshold.</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/error_metrics_vs_epsEnd.png\"\n",
    "       alt=\"error_metrics_vs_epsEnd\"\n",
    "       style=\"width:80%; max-width:800px; border:1px solid #ccc;\">\n",
    "  <br>\n",
    "  <em>Figure 7: Error metrics versus relative worst-case error threshold.</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Analysis and Verification of Generated Approximate Multiplier\n",
    "\n",
    "In this section, we map the generated netlist to a specific semiconductor technology (GF180, for example) and perform functional verification as well as power, performance, and area (PPA) analysis using OpenSTA. The design is mappd to standard cells from the target technology library, and timing, power, and area reports are extracted to evaluate the quality of the evolved approximate multiplier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Netlist Mapping to GF180 Structural Verilog\n",
    "\n",
    "This step converts the evolved netlist produced by the genetic algorithm into a structural SystemVerilog format compatible with the GF180MCU standard-cell library. The script calls the Python utility netlist_to_verilog.py, which reads the raw gate-level description (netlist_output.txt) and translates it into a clean, synthesizable SystemVerilog module. The resulting file netlist_output.sv is saved under code/genetic_algorithm/output/ and the top-level module is named approxMult_signed8x8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvCEBjWwLwBi"
   },
   "outputs": [],
   "source": [
    "# === 5.1. Netlist Mapping to GF180 Structural Verilog ===\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "print('Mapping generated netlist to structural SystemVerilog...')\n",
    "subprocess.run([sys.executable, 'scripts/netlist_to_verilog.py', '--in', './code/genetic_algorithm/output/netlist_output.txt', '--out', './code/genetic_algorithm/output/netlist_output.sv', '--module-name', 'approxMult_signed8x8'], check=True)\n",
    "print('Mapped netlist written to code/genetic_algorithm/output/netlist_output.sv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Static Timing Analysis for the Evolved Approximate Design\n",
    "\n",
    "This step performs static timing analysis (STA) on the evolved approximate multiplier using OpenSTA, ensuring timing integrity after the design evolution. The script checks whether the sta binary (OpenSTA) is available in the system path. If found, it runs the timing script sta_approx.tcl under code/synthesis/sta/, using the same GF180MCU timing library (gf180mcu_fd_sc_mcu9t5v0__tt_025C_3v30.lib). All analysis results are written to sta_approx.report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5.2. Static Timing Analysis for the Evolved Approximate Design ===\n",
    "\n",
    "sta_bin = shutil.which('sta')\n",
    "if sta_bin:\n",
    "    Path('code/synthesis/sta').mkdir(parents=True, exist_ok=True)\n",
    "    env = os.environ.copy(); env['LIB_SYN'] = GF_LIB\n",
    "    with open('code/synthesis/sta/sta_approx.report', 'w') as fout:\n",
    "        subprocess.run([sta_bin, 'code/synthesis/sta/sta_approx.tcl'], check=True, stdout=fout, env=env)\n",
    "    print('[STA] Done. Reports in code/synthesis/sta')\n",
    "else:\n",
    "    print('[WARN] OpenSTA (sta) not found; skipping STA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JHQNM15WW2K"
   },
   "source": [
    "### 5.3 Gate-Level Simulation and Truth Table Extraction of the Evolved Design\n",
    "\n",
    "This step verifies the functionality of the evolved approximate multiplier through gate-level simulation, followed by truth table extraction for quantitative error analysis.\n",
    "\n",
    "5.31 Gate-Level Simulation\n",
    "\n",
    "The script compiles and runs the evolved SystemVerilog netlist (netlist_output.sv) together with its dedicated testbench (approxMult_signed8x8_tb.sv) using Icarus Verilog (iverilog + vvp). It links against the GF180MCU standard-cell models (gf180mcu_fd_sc_mcu9t5v0.v, primitives.v) to accurately model technology-specific behavior. Simulation results are stored in gl_sim_output.txt, and the waveform is dumped into gl.vcd for inspection.\n",
    "\n",
    "5.32 Truth Table Extraction\n",
    "\n",
    "After simulation, the script calls extract_truth_table.py to parse the simulation results and reconstruct the complete input–output truth table of the evolved multiplier.\n",
    "This extracted table serves as the basis for evaluating numerical error metrics such as ER, WCE, NMED, and MRE, enabling precise comparison against the golden reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF3CYKLSLx-9"
   },
   "outputs": [],
   "source": [
    "# === 5.3. Gate-Level Simulation and Truth Table Extraction of the Evolved Design ===\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "VSIM_DIR = Path('code/synthesis/vsim')\n",
    "VSIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STD_CELLS = [str(Path('src') / 'gf180mcu_fd_sc_mcu9t5v0.v'), str(Path('src') / 'primitives.v')]\n",
    "APPROX_NETLIST = 'code/genetic_algorithm/output/netlist_output.sv'\n",
    "APPROX_NETLIST_TESTBENCH = 'code/synthesis/verilog/testbench/approxMult_signed8x8_tb.sv'\n",
    "GL_VVP = str(VSIM_DIR / 'gl.vvp')\n",
    "GL_VCD = str(VSIM_DIR / 'gl.vcd')\n",
    "\n",
    "# -- 5.31 Gate-Level Simulation --\n",
    "iverilog_cmd = ['iverilog', '-g2012', '-I../verilog/modules', '-o', GL_VVP] + STD_CELLS + [APPROX_NETLIST, APPROX_NETLIST_TESTBENCH, '-D', f'VSIM_OUT=\"{VSIM_DIR}/gl_sim_output.txt\"', '-D', f'VCD_FILE=\"{GL_VCD}\"']\n",
    "print('Compiling approximate netlist simulation...')\n",
    "subprocess.run(iverilog_cmd, check=True)\n",
    "subprocess.run(['vvp', GL_VVP], check=True)\n",
    "print('Simulation finished; extracting truth table...')\n",
    "\n",
    "# -- 5.32 Truth Table Extraction --\n",
    "subprocess.run([sys.executable, 'scripts/extract_truth_table.py'], check=True)\n",
    "print('Truth table extraction complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo6WGc9_XAoK"
   },
   "source": [
    "### 5.4 Error Evaluation on Extracted Truth Table\n",
    "\n",
    "This step performs quantitative error analysis on the evolved approximate multiplier based on its extracted truth table. The script checks for the presence of the file extracted_truth_table.txt, generated in the previous step. If it exists, it calls error_eval.py to compute various approximation error metrics, using 16384 as the normalization factor (the maximum absolute value for an 8-bit signed multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckTzEyPJH1Sh"
   },
   "outputs": [],
   "source": [
    "# === 5.4. Error Evaluation on Extracted Truth Table ===\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "tt = Path('code/genetic_algorithm/output/extracted_truth_table.txt')\n",
    "if tt.exists():\n",
    "    print('Running error evaluation on extracted truth table...')\n",
    "    subprocess.run([sys.executable, 'scripts/error_eval.py', str(tt), '--maxabs', '16384'], check=True)\n",
    "else:\n",
    "    print(f'Extracted truth table not found at {tt}; run the simulation/extraction step first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Implementation of Deep Learning Tasks: ResNet-18 & ResNet-20 with Approximate Computing Units\n",
    "### 6.1 Fine-tune an INT8 quantized ResNet-20 on the CIFAR-100 dataset\n",
    "We start by fine-tuning an INT8-quantized ResNet-20 on the CIFAR-100 dataset. This experiment serves as a proof-of-concept for integrating approximate arithmetic units within a complete image-classification pipeline. It establishes the baseline workflow and verifies functional compatibility before extending the approach to larger architectures such as ResNet-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization-aware training for CIFAR-100 ResNet-20 using helpers from train_qat_cifar100_resnet20.py\n",
    "import os, sys, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "module_dir = Path(\"code/resnet/quant_code\")\n",
    "if not module_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Expected directory '{module_dir}' alongside this notebook.\")\n",
    "if str(module_dir.resolve()) not in sys.path:\n",
    "    sys.path.insert(0, str(module_dir.resolve()))\n",
    "repo_root = Path(\".\").resolve() / \"code\" / \"resnet\"\n",
    "\n",
    "from train_qat_cifar100_resnet20 import (\n",
    "    Cfg, get_loaders_cifar100, QResNet20CIFAR,\n",
    "    train_one_epoch, evaluate, set_activation_quant_enabled,\n",
    "    calibrate_activations, calibrate_weights\n",
    " )\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "cfg = Cfg()\n",
    "cfg.data_root = str((repo_root / \"datasets\").resolve())\n",
    "cfg.fp32_ckpt = str((repo_root / \"runs_fp32\" / \"resnet20_cifar100_fp32.pt\").resolve())\n",
    "cfg.out_dir = str((repo_root / \"runs_qat\" / \"resnet20\").resolve())\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "train_loader, test_loader = get_loaders_cifar100(cfg)\n",
    "model = QResNet20CIFAR(num_classes=cfg.num_classes, cfg=cfg).to(cfg.device)\n",
    "\n",
    "if cfg.fp32_ckpt and os.path.isfile(cfg.fp32_ckpt):\n",
    "    sd = torch.load(cfg.fp32_ckpt, map_location='cpu')\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    print(f'load_state: missing={len(missing)} unexpected={len(unexpected)}')\n",
    "else:\n",
    "    print('[Warn] fp32 checkpoint not found, training continues from scratch.')\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=cfg.lr_qat, momentum=cfg.momentum,\n",
    "    weight_decay=cfg.weight_decay, nesterov=cfg.nesterov\n",
    " )\n",
    "\n",
    "total_epochs = cfg.warmup_epochs + cfg.epochs_qat\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs, eta_min=cfg.eta_min)\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "\n",
    "set_activation_quant_enabled(model, in_enabled=False, out_enabled=False)\n",
    "print(f'[Warm-up] epochs={cfg.warmup_epochs}')\n",
    "for ep in range(1, cfg.warmup_epochs + 1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, cfg.device)\n",
    "    te_loss, te_acc = evaluate(model, test_loader, cfg.device)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'[W{ep:02d}] lr {cur_lr:.2e} | train {tr_loss:.4f}/{tr_acc*100:.2f}% | test {te_loss:.4f}/{te_acc*100:.2f}%')\n",
    "    scheduler.step()\n",
    "\n",
    "calibrate_weights(model)\n",
    "\n",
    "set_activation_quant_enabled(model, in_enabled=cfg.quantize_input, out_enabled=cfg.quantize_output)\n",
    "print(f'[Calib-1] collecting {cfg.calib_steps} mini-batches...')\n",
    "calibrate_activations(model, train_loader, cfg.device, cfg.calib_steps)\n",
    "\n",
    "total_qat = cfg.epochs_qat\n",
    "re_ep = int(total_qat * cfg.recalib_ratio) if cfg.recalib else -1\n",
    "\n",
    "for ep in range(1, total_qat + 1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, cfg.device)\n",
    "    te_loss, te_acc = evaluate(model, test_loader, cfg.device)\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"test_loss\"].append(te_loss);  history[\"test_acc\"].append(te_acc)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'[QAT {ep:03d}] lr {cur_lr:.2e} | train {tr_loss:.4f}/{tr_acc*100:.2f}% | test {te_loss:.4f}/{te_acc*100:.2f}%')\n",
    "    scheduler.step()\n",
    "    if cfg.recalib and ep == re_ep:\n",
    "        print(f'[Calib-2 @ QAT {ep}] collecting {cfg.calib_steps} mini-batches...')\n",
    "        calibrate_activations(model, train_loader, cfg.device, cfg.calib_steps)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(cfg.out_dir, cfg.ckpt))\n",
    "with open(os.path.join(cfg.out_dir, cfg.hist), 'w') as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "te_loss, te_acc = evaluate(model, test_loader, cfg.device)\n",
    "print(f'INT8 final: loss {te_loss:.4f}, acc {te_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Diagnostic: INT8 Weights with an Approximate Multiplier\n",
    "\n",
    "We ran a control diagnostic where we kept the baseline INT8 weights unchanged and only swapped the exact multiplier for an approximate one. Classification accuracy degraded sharply. This confirms that hardware-aware fine-tuning is necessary: quantized weights alone cannot offset the additional arithmetic error introduced by approximate units.\n",
    "\n",
    "Implementation note. For each approximate-multiplier variant, point lut_table_path to the matching LUT so the kernel uses the correct truth table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import time, torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from resnet20_lut import ResNet20LUTCfg, build_lut_resnet20, resolve_lut_table\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Cfg(ResNet20LUTCfg):\n",
    "    data_root: str = 'code/resnet/datasets'\n",
    "    test_subset: int | None = None\n",
    "    lut_table_path: str | None = 'code/resnet/lut/truth_table_0.5.csv'\n",
    "    qat_ckpt: str = 'code/resnet/runs_qat/resnet20/resnet20_cifar100_qat_int8.pt'\n",
    "    # lut_table_path: str | None = None\n",
    "\n",
    "\n",
    "\n",
    "def get_test_loader(cfg: Cfg) -> DataLoader:\n",
    "    mean = [0.5071, 0.4865, 0.4409]\n",
    "    std = [0.2673, 0.2564, 0.2762]\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    data_root = Path(cfg.data_root).expanduser()\n",
    "    data_root.mkdir(parents=True, exist_ok=True)\n",
    "    test_set = datasets.CIFAR100(str(data_root), train=False, download=True, transform=test_tf)\n",
    "    if cfg.test_subset is not None and cfg.test_subset < len(test_set):\n",
    "        indices = torch.arange(cfg.test_subset)\n",
    "        test_set = Subset(test_set, indices)\n",
    "    return DataLoader(test_set, batch_size=cfg.batch_sz, shuffle=False,\n",
    "                      num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: str) -> tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if device.startswith('cuda'):\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    total_batches = len(loader)\n",
    "    for step, (x, y) in enumerate(loader, 1):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        if step % max(1, total_batches // 10) == 0 or step == total_batches:\n",
    "            print(f'[Eval] processed {step}/{total_batches} batches...')\n",
    "    if device.startswith('cuda'):\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.time() - t0\n",
    "    avg_loss = total_loss / max(total, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    return avg_loss, acc, elapsed\n",
    "\n",
    "cfg = Cfg()\n",
    "device = cfg.device\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if cfg.lut_table_path is not None:\n",
    "    base_dir = Path.cwd()\n",
    "    resolve_lut_table(cfg, base_dir)\n",
    "    print(f'[Info] Loaded LUT truth table: {cfg.lut_table_path}')\n",
    "\n",
    "print('[Info] Building LUT ResNet-20 and loading QAT checkpoint...')\n",
    "print(f'[Info] Using checkpoint at: {cfg.qat_ckpt}')\n",
    "model = build_lut_resnet20(cfg).to(device)\n",
    "\n",
    "print('[Info] Preparing CIFAR-100 test set...')\n",
    "test_loader = get_test_loader(cfg)\n",
    "\n",
    "print('[Info] Starting evaluation...')\n",
    "loss, acc, elapsed = evaluate(model, test_loader, device)\n",
    "samples = len(test_loader.dataset)\n",
    "print(f'[Result] test_loss={loss:.4f}, test_acc={acc*100:.2f}%, time={elapsed:.2f}s, throughput={samples/elapsed:.1f} samples/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Hardware-Aware Fine-Tuning with Approximate Multipliers\n",
    "\n",
    "We now proceed to hardware-aware fine-tuning. During backpropagation, the gradient of the exact multiplier is substituted for that of the approximate multiplier to preserve a descent direction and ensure that training remains stable. Although the forward pass uses non-differentiable LUT-based approximate multipliers, this gradient substitution allows the network to adapt its parameters to compensate for arithmetic inaccuracies.\n",
    "\n",
    "\n",
    "\n",
    "This stage aligns algorithmic adaptation with hardware characteristics, yielding stable convergence and near-optimal accuracy under approximate computation.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/fine_tuning_explanation.png\"\n",
    "       alt=\"Fine tuning\"\n",
    "       style=\"width:80%; max-width:800px; border:1px solid #ccc;\">\n",
    "  <br>\n",
    "  <em>Figure 8: Fine tuning for Approx Mult..</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "module_dir = Path(\"code/resnet/quant_code\")\n",
    "if not module_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Expected directory '{module_dir}' alongside this notebook.\")\n",
    "module_path = str(module_dir.resolve())\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from resnet20_lut import (\n",
    "    QResNet20CIFARLUT,\n",
    "    ResNet20LUTCfg,\n",
    "    build_lut_resnet20,\n",
    "    resolve_lut_table,\n",
    " )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Cfg(ResNet20LUTCfg):\n",
    "    qat_ckpt: str = 'code/resnet/runs_qat/resnet20/resnet20_cifar100_qat_int8.pt'  # QAT weight path\n",
    "    data_root: str = 'code/resnet/datasets'\n",
    "    epochs: int = 5\n",
    "    lr: float = 5e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    momentum: float = 0.9\n",
    "    nesterov: bool = True\n",
    "    log_interval: int = 100\n",
    "    eta_min: float = 1e-5\n",
    "    output_dir: str = 'code/resnet/runs_qat/resnet20'\n",
    "    save_path: str | None = None\n",
    "    resume_path: str | None = None\n",
    "    history_path: str | None = None\n",
    "    lut_table_path: str | None = 'code/resnet/lut/truth_table_0.5.csv'\n",
    "    num_workers: int = 0\n",
    "\n",
    "\n",
    "def get_loaders(cfg: Cfg) -> tuple[DataLoader, DataLoader]:\n",
    "    mean = [0.5071, 0.4865, 0.4409]\n",
    "    std = [0.2673, 0.2564, 0.2762]\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    train_set = datasets.CIFAR100(cfg.data_root, train=True, download=True, transform=train_tf)\n",
    "    test_set = datasets.CIFAR100(cfg.data_root, train=False, download=True, transform=test_tf)\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=cfg.batch_sz, shuffle=True, num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=cfg.batch_sz, shuffle=False, num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def infer_lut_tag(lut_path: str | None) -> str:\n",
    "    if not lut_path:\n",
    "        return 'exact'\n",
    "    stem = os.path.splitext(os.path.basename(lut_path))[0]\n",
    "    candidate = stem.split('_')[-1] if '_' in stem else stem\n",
    "    candidate = candidate.strip()\n",
    "    return candidate or 'custom'\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: QResNet20CIFARLUT,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    log_interval: int,\n",
    " ) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    for step, (x, y) in enumerate(loader, 1):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        if step % max(1, log_interval) == 0:\n",
    "            print(\n",
    "                f'[Train] step={step:04d} loss={loss.item():.4f} acc={(correct/total)*100:.2f}%',\n",
    "                flush=True,\n",
    "            )\n",
    "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: QResNet20CIFARLUT, loader: DataLoader, device: str) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "\n",
    "def build_model(cfg: Cfg) -> QResNet20CIFARLUT:\n",
    "    if cfg.qat_ckpt and os.path.isfile(cfg.qat_ckpt):\n",
    "        model = build_lut_resnet20(cfg)\n",
    "    elif cfg.resume_path:\n",
    "        print('[Warn] Initial QAT checkpoint not found, falling back to resume only.')\n",
    "        model = QResNet20CIFARLUT(num_classes=cfg.num_classes, cfg=cfg)\n",
    "    else:\n",
    "        raise FileNotFoundError('Missing initial QAT weights; set cfg.qat_ckpt or provide resume_path')\n",
    "    if cfg.resume_path and os.path.isfile(cfg.resume_path):\n",
    "        print(f'[Info] Resume from {cfg.resume_path}')\n",
    "        state = torch.load(cfg.resume_path, map_location='cpu')\n",
    "        model.load_state_dict(state, strict=False)\n",
    "    return model\n",
    "\n",
    "cfg = Cfg()\n",
    "device = cfg.device\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if cfg.lut_table_path is not None:\n",
    "    base_dir = Path.cwd()\n",
    "    resolve_lut_table(cfg, base_dir)\n",
    "    print(f'[Info] Loaded LUT table: {cfg.lut_table_path}')\n",
    "lut_tag = infer_lut_tag(cfg.lut_table_path)\n",
    "if cfg.save_path is None:\n",
    "    cfg.save_path = os.path.join(cfg.output_dir, f'resnet20_cifar100_qat_int8_lut_{lut_tag}_ft.pt')\n",
    "if cfg.history_path is None:\n",
    "    cfg.history_path = os.path.join(cfg.output_dir, f'history_ft_resnet20_lut_{lut_tag}.json')\n",
    "train_loader, test_loader = get_loaders(cfg)\n",
    "model = build_model(cfg).to(device)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay, nesterov=cfg.nesterov\n",
    " )\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs, eta_min=cfg.eta_min)\n",
    "best_acc = 0.0\n",
    "history: dict[str, list[float]] = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "}\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, cfg.log_interval)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - t0\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}] {elapsed:.1f}s | train {train_loss:.4f}/{train_acc*100:.2f}% | \"\n",
    "        f\"test {test_loss:.4f}/{test_acc*100:.2f}% | lr={optimizer.param_groups[0]['lr']:.2e}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        os.makedirs(os.path.dirname(cfg.save_path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), cfg.save_path)\n",
    "        print(f'[Info] Saved new best model -> {cfg.save_path} (acc={best_acc*100:.2f}%)', flush=True)\n",
    "print(f'[Done] Best test accuracy: {best_acc*100:.2f}%')\n",
    "os.makedirs(os.path.dirname(cfg.history_path), exist_ok=True)\n",
    "with open(cfg.history_path, 'w') as f:\n",
    "    json.dump(history, f)\n",
    "print(f'[Info] Training history saved to {cfg.history_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Impact of Multiplier Accuracy on Fine-Tuning Performance\n",
    "\n",
    "To evaluate the impact of multiplier accuracy, we perform hardware-aware fine-tuning using approximate multipliers characterized by different Relative Worst-Case Error (WCE) values.\n",
    "Figure right illustrates the test loss, and Figure left shows the test accuracy over 12 epochs for each configuration.\n",
    "\n",
    "Observations\n",
    "\n",
    "Low-error multipliers (Relative WCE ≤ 0.02) achieve stable convergence and maintain accuracy close to the exact baseline (≈ 68–70%).\n",
    "\n",
    "Moderate-error multipliers (Relative WCE ≈ 0.05) still allow partial recovery through fine-tuning, reaching about 55–60% accuracy.\n",
    "\n",
    "High-error multipliers (Relative WCE ≥ 0.1) show significantly degraded convergence and accuracy, indicating that when arithmetic error becomes large, gradient substitution alone cannot fully compensate for the approximate behavior.\n",
    "\n",
    "These results demonstrate that hardware-aware fine-tuning can successfully adapt to approximate arithmetic units with small to moderate error levels, but excessive deviation from exact computation leads to accuracy collapse.\n",
    "\n",
    "Fairness and Reproducibility\n",
    "\n",
    "For a fair comparison, all experiments are fine-tuned for exactly 12 epochs under identical hyperparameters and optimization schedules.\n",
    "Readers interested in further exploration may extend the fine-tuning process or adjust the learning-rate decay to investigate whether additional training epochs can further recover performance for higher-error multipliers.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/lut_compare_acc.png\"\n",
    "       alt=\"Test Accuracy for Approximate Multipliers\"\n",
    "       style=\"width:45%; max-width:500px; border:1px solid #ccc; margin-right:10px;\">\n",
    "  <img src=\"./images/lut_compare_loss.png\"\n",
    "       alt=\"Test Loss for Approximate Multipliers\"\n",
    "       style=\"width:45%; max-width:500px; border:1px solid #ccc;\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <em>Figure 9: Test accuracy and loss during hardware-aware fine-tuning with approximate multipliers of different relative WCE values.</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the logs to see the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_path = \"runs_qat/resnet20/history_qat_resnet20.json\"   \n",
    "# ISSCC26/CircuitsDNA/code/resnet/runs_qat/resnet20/history_ft_resnet20_lut_0.5.json\n",
    "save_path    = \"runs_qat/resnet20/history_qat_resnet20.json.png\"     \n",
    "title        = \"Training Curves\"\n",
    "smooth_alpha = 0.2  # Exponential smoothing parameters, 0 or None indicate that no smoothing is used.\n",
    "\n",
    "\n",
    "def _resolve_alpha(alpha):\n",
    "    if alpha is None:\n",
    "        return None\n",
    "    try:\n",
    "        alpha = float(alpha)\n",
    "    except (TypeError, ValueError):\n",
    "        return None\n",
    "    if alpha <= 0:\n",
    "        return None\n",
    "    return alpha\n",
    "\n",
    "\n",
    "ema_alpha = _resolve_alpha(smooth_alpha)\n",
    "raw_alpha = 0.35 if ema_alpha else 0.9\n",
    "\n",
    "def ema(xs, alpha=0.2):\n",
    "    if xs is None: return None\n",
    "    out = []\n",
    "    m = None\n",
    "    for v in xs:\n",
    "        m = v if m is None else (alpha * v + (1 - alpha) * m)\n",
    "        out.append(m)\n",
    "    return out\n",
    "\n",
    "def load_history(p):\n",
    "    with open(p, \"r\") as f:\n",
    "        raw = json.load(f)\n",
    "    keys = [\"train_loss\", \"test_loss\", \"train_acc\", \"test_acc\"]\n",
    "    if isinstance(raw, list):\n",
    "        hist = {k: [] for k in keys}\n",
    "        for entry in raw:\n",
    "            for k in keys:\n",
    "                hist[k].append(entry.get(k, 0.0))\n",
    "        return hist\n",
    "    for k in keys:\n",
    "        raw.setdefault(k, [])\n",
    "    return raw\n",
    "\n",
    "history = load_history(history_path)\n",
    "epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n",
    "\n",
    "TL, TeL = history[\"train_loss\"], history[\"test_loss\"]\n",
    "TA, TeA = history[\"train_acc\"], history[\"test_acc\"]\n",
    "\n",
    "if ema_alpha:\n",
    "    TLs, TeLs = ema(TL, ema_alpha), ema(TeL, ema_alpha)\n",
    "    TAs, TeAs = ema(TA, ema_alpha), ema(TeA, ema_alpha)\n",
    "else:\n",
    "    TLs, TeLs, TAs, TeAs = TL, TeL, TA, TeA\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, TL,  label=\"Train Loss\", alpha=raw_alpha)\n",
    "plt.plot(epochs, TeL, label=\"Test Loss\",  alpha=raw_alpha)\n",
    "if ema_alpha:\n",
    "    plt.plot(epochs, TLs, label=f\"Train Loss (EMA {ema_alpha:.2f})\", alpha=1.0)\n",
    "    plt.plot(epochs, TeLs, label=f\"Test Loss (EMA {ema_alpha:.2f})\", alpha=1.0)\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n",
    "plt.grid(True); plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, [a*100 for a in TA],  label=\"Train Acc (%)\", alpha=raw_alpha)\n",
    "plt.plot(epochs, [a*100 for a in TeA], label=\"Test Acc (%)\",  alpha=raw_alpha)\n",
    "if ema_alpha:\n",
    "    plt.plot(epochs, [a*100 for a in TAs],  label=f\"Train Acc (EMA {ema_alpha:.2f})\", alpha=1.0)\n",
    "    plt.plot(epochs, [a*100 for a in TeAs], label=f\"Test Acc (EMA {ema_alpha:.2f})\", alpha=1.0)\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Accuracy\")\n",
    "plt.grid(True); plt.legend()\n",
    "\n",
    "plt.suptitle(title)\n",
    "plt.tight_layout(rect=[0,0,1,0.96])\n",
    "\n",
    "if save_path:\n",
    "    os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=180)\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readers can refer to the scripts in `code/quant_code` to fine-tune larger networks—such as ResNet-18—under the approximate multiplier configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Currently, only six types of gates are used for synthesis and optimization, which may limit the design space explored by Yosys-abc. This restriction could lead to suboptimal implementations, especially for more complex arithmetic circuits where richer logic primitives can significantly reduce depth and transistor count. However, the proposed approach is fundamentally general and easily extensible. By modifying the gate set, our framework can seamlessly incorporate new logic types or even higher-level functional blocks such as full adders (FA), multiplexers, barrel shifters, etc. This flexibility allows the evolutionary algorithm to search a vastly larger and more expressive design space, potentially discovering architectures that are both more compact and more accurate. In other words, while our current experiments focus on a limited set of gates for clarity and consistency, the methodology itself provides a scalable foundation for future work that targets richer, application-specific gate libraries or hierarchical circuit components.\n",
    "\n",
    "2. From our previous experiments, we observed that the network size has a significant impact on the accuracy after replacing standard units with approximate computing elements. Larger networks tend to suffer from greater accuracy degradation due to cumulative errors introduced by multiple approximate operations. However, for the same task, larger networks also exhibit stronger error resilience through fine-tuning, they can effectively compensate for these approximation-induced errors and ultimately achieve better performance than smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusion\n",
    "\n",
    "In this work, we presented a comprehensive framework that bridges evolutionary circuit synthesis and approximate deep learning acceleration. At the circuit level, we developed a genetic algorithm-based approximate synthesis flow that optimizes logic structures under multiple constraints. Only six basic gates were used in the current setup to ensure simplicity and compatibility with standard cell libraries, but the framework itself is general and extensible—it can easily incorporate more complex gate types or higher-level functional blocks such as full adders or barrel shifters, thus broadening the search space for better trade-offs.\n",
    "At the algorithmic level, we integrated these evolved approximate multipliers into neural network inference pipelines and evaluated their impact on real-world tasks.\n",
    "\n",
    "Overall, the proposed framework establishes a complete design loop from transistor-level logic optimization to system-level accuracy evaluation. It provides a powerful foundation for co-exploring accuracy, efficiency, and resilience across multiple abstraction layers. Future work will focus on extending the gate library, incorporating realistic physical constraints (timing, power, PVT variation), and exploring joint hardware–software co-design strategies for energy-efficient AI accelerators."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
