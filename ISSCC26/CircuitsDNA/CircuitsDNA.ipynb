{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1pJcY6eSMd5"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1142VbFt8kLRz7amDi9UQt5LaJvIcTaZt?usp=sharing\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RT4F8tQclTwl"
   },
   "source": [
    "# CircuitsDNA - Approximate Circuit Synthesis based on Evolutionary Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzxIs9VeZ3Cj"
   },
   "source": [
    "```\n",
    "Submission to IEEE SSCS Open-Source Ecosystem “Code-a-Chip” Travel Grant Awards at ISSCC'26\n",
    "SPDX-License-Identifier: GPL-3.0-only\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFISj7qnZ01A"
   },
   "source": [
    "|Name|Affiliation|IEEE Member|SSCS Member|\n",
    "|:--:|:----------:|:----------:|:----------:|\n",
    "|Ruichen Qi <br /> Email ID: ruichen_qi@brown.edu|Brown University|No|No|\n",
    "|Junyi Luo <br /> Email ID: junyi_luo@brown.edu|Brown University|No|No|\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfN6vNNNl9fY"
   },
   "source": [
    "## 1. Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAteluC1JJF-"
   },
   "source": [
    "### 1.1 Background\n",
    "\n",
    "According to Moore’s Law and Dennard scaling, continuous transistor miniaturization since 1974 has enabled exponential growth in device density—doubling with each generation—while maintaining higher clock speeds under constant power density. However, around 2007, the benefits of Dennard scaling began to fade due to leakage currents and power density constraints, and by 2012, scaling had largely halted.\n",
    "\n",
    "Modern computing systems now face severe power and thermal bottlenecks. As transistor density continues to rise, heat dissipation has become a fundamental limitation: chips can no longer operate all transistors simultaneously without exceeding safe thermal limits. This results in “dark silicon,” where only a portion of the available cores can remain active to avoid overheating. Elevated temperatures also degrade device reliability, accelerating wear-out mechanisms and shortening system lifetime.\n",
    "\n",
    "To address these challenges, approximate computing has emerged as a promising solution. By relaxing accuracy requirements in error-tolerant applications, approximate designs can substantially reduce power consumption and heat generation while maintaining acceptable output quality. This paradigm is especially suitable for neural network and large language model (LLM) accelerators, which are inherently resilient to small arithmetic errors. Minor inaccuracies in multiplications or accumulations typically have negligible impact on model accuracy, allowing designers to adopt approximate multipliers, adders, or reduced-precision datapaths for significant savings in power, area, and latency.\n",
    "\n",
    "Moreover, fine-tuning techniques can further mitigate hardware-induced errors. By retraining or adapting model parameters on the approximate hardware, most of the lost accuracy can be recovered while retaining energy efficiency gains. This makes approximate computing particularly attractive for large-scale AI accelerators, where computational and memory demands are exceptionally high.\n",
    "\n",
    "### 1.2 Motivation\n",
    "\n",
    "Despite its promise, approximate computing still faces practical challenges in logic synthesis. Traditional design methods—such as manual simplification or heuristic gate pruning often rely on structural assumptions and struggle to scale for complex arithmetic blocks like multipliers. These methods typically yield locally optimized solutions and lack the flexibility to explore the vast combinational design space.\n",
    "\n",
    "To overcome these limitations, genetic algorithms (GAs) offer an effective alternative. By evolving populations of candidate circuits through mutation, crossover, and selection, GAs can efficiently explore discrete, non-linear design spaces without requiring gradient information or explicit models. Their ability to support multi-objective optimization makes them ideal for balancing trade-offs among accuracy, power, area, and delay.\n",
    "\n",
    "However, most GA-based approximate synthesis approaches remain limited to small-scale demonstrations and rarely connect circuit-level optimization to system-level evaluation. This work bridges that gap by introducing an end-to-end GA-driven framework that synthesizes approximate computing circuits and evaluates their real-world impact on neural network tasks such as image classifications\n",
    "\n",
    "### 1.3 Notebook Overview\n",
    "\n",
    "This notebook presents a genetic-algorithm–based framework for approximate logic synthesis and its application-level evaluation on deep learning models including Fashion-MNIST (LeNet-5) and CIFAR-100 (ResNet-18/ResNet-20).\n",
    "The framework supports both random circuit generation and optimization from a seed netlist, providing a complete flow from synthesis to performance analysis.\n",
    "\n",
    "Workflow summary:\n",
    "\n",
    "An 8-bit signed multiplier is synthesized and verified using Yosys-ABC, iVerilog, and OpenSTA.\n",
    "\n",
    "The genetic algorithm performs approximate logic synthesis on the extracted netlist under various error and area constraints.\n",
    "\n",
    "The evolved approximate design is analyzed in OpenSTA to extract power, timing, and LUT information.\n",
    "\n",
    "Finally, the approximate multipliers are integrated into deep learning workloads (LeNet-5, ResNet-18, and ResNet-20) to evaluate accuracy–efficiency trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FIgure] Workflow of our algorithm and approx multiplier based systolic array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4qdBSkHKfFp"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Setting up of Open Source Tools\n",
    "\n",
    "The latest versions of Yosys, iVerilog, and OpenSTA should be installed. There are many installation tutorials available online. You can either install them manually by following the instructions on their GitHub pages, or use the script we’ve provided:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ergg8MtZUKvi"
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "!make create_env\n",
    "!make check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpQ-LwCtKcNg",
    "outputId": "fc9a93b9-7953-44a7-db61-aaa7229234a5"
   },
   "outputs": [],
   "source": [
    "!python -m venv venv\n",
    "!venv/bin/python -m pip install --upgrade pip ipykernel\n",
    "!venv/bin/python -m pip install -r requirements.txt\n",
    "!venv/bin/python -m ipykernel install --user --name=venv --display-name \"Python (venv)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gedLMGoWcp2E"
   },
   "source": [
    "For the following steps, please switch the python kernal to the virtual environment we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rb6QWz2LOBu"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. 8-bit Signed Multiplier Synthesis and Verification\n",
    "\n",
    "In this demonstration, an 8-bit signed multiplier is synthesized and verified based on the GF180 technology as an example. All necessary files for synthesis and verification are prepared. Only six basic gate types — AND, NAND, OR, XOR, XNOR, and INV are used for synthesis, though the framework can be easily extended to include other logic gates or larger building blocks such as full adders (FA), barrel shifters, multipliers, etc. Here, we use these six gate types solely as a representative example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O7R6XsXx9DR"
   },
   "source": [
    "Goldenbrick generation and behavior simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnGPI5zaLM9j"
   },
   "outputs": [],
   "source": [
    "# Goldenbrick generation\n",
    "!make goldenbrick_gen\n",
    "\n",
    "# Behavior simulation\n",
    "!make sim_behavior\n",
    "!make check_behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesis, post-synthesis simulation and verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis with a target frequency of 100MHz\n",
    "!make run_synth\n",
    "\n",
    "# Post-synthesis simulation and verification\n",
    "!make sim_synth\n",
    "!make check_synth\n",
    "\n",
    "# Static Timing Analysis (STA)\n",
    "!make sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_K1JaNfLmkV"
   },
   "source": [
    "---\n",
    "## 4. Approximate Multiplier Synthesis from the Synthesized Netlist via Genetic Algorithm\n",
    "\n",
    "In this section, we use a genetic algorithm (GA) to evolve approximate 8-bit signed multipliers. Genetic algorithms struggle with large circuits due to the exponentially growing search space, high computational cost, and poor scalability. Therefore, our GA starts from the seed circuit, which is loaded from the synthesized Verilog netlist in previous step.\n",
    "\n",
    "During each generation, the algorithm evaluates all candidate circuits by simulating their outputs over all input patterns. The fitness function jointly considers circuit accuracy and area efficiency — penalizing individuals with large worst-case errors (WCE > ε_th) while favoring smaller transistor counts. Multiple error metrics such as NMED, ER, WCE, MRE, and sMAPE are recorded for analysis.\n",
    "\n",
    "New individuals are created through mutation operators, including:\n",
    "\n",
    "Add / delete node: randomly insert or remove a gate;\n",
    "\n",
    "Change gate type: switch to another logic primitive;\n",
    "\n",
    "Rewire inputs or outputs: alter signal connectivity;\n",
    "\n",
    "Merge equivalent nodes: remove redundant subcircuits.\n",
    "\n",
    "By iteratively applying mutation, pruning inactive nodes, and selecting the best individuals, the GA searches the discrete, irregular design space to obtain compact approximate multipliers with bounded output error. This process enables automatic approximate logic synthesis directly at the gate level, providing a flexible framework for exploring accuracy–area trade-offs under the GF180 technology library.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FIgure] Algorithm explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTQ4fcrMDQxO"
   },
   "source": [
    "Since this algorithm is implemented using Cpp for acceleration, firstly we need to compile the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBfr9hpuLg16"
   },
   "outputs": [],
   "source": [
    "# Compile the GA code\n",
    "!make compile_ga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5tb4EUFSix2"
   },
   "source": [
    "Then we're ready to evolve approximate logics using this algorithm. The evolution may take several hours to finish, depending on the performance of current platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDmtJXr-Lshl"
   },
   "outputs": [],
   "source": [
    "# Start the GA evolution\n",
    "!make run_ga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the evolution with an relative worst case error threshold of 0.5% as an example. All the experiments in this section were run on an AMD Ryzen 9 7845HX laptop for 14 hours.\n",
    "\n",
    "<img src=\"images/evolution_process_epsEnd_0.005.png\"\n",
    "     alt=\"Evolution Process\"\n",
    "     style=\"width:800px;max-width:100%;border:1px solid #ccc;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid search over the relative worst-case error threshold was performed, and the results are shown below. With only a 0.5% relative worst-case error, the equivalent transistor count was reduced to 76.5% of the original size.\n",
    "\n",
    "<img src=\"images/equivalent_transistor_count_vs_epsEnd.png\"\n",
    "     alt=\"Evolution Process\"\n",
    "     style=\"width:800px;max-width:100%;border:1px solid #ccc;\">\n",
    "\n",
    "<img src=\"images/error_metrics_vs_epsEnd.png\"\n",
    "     alt=\"Evolution Process\"\n",
    "     style=\"width:800px;max-width:100%;border:1px solid #ccc;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Analysis and Verification of Generated Approximate Multiplier\n",
    "\n",
    "In this section, we map the generated netlist to a specific semiconductor technology (GF180, for example) and perform functional verification as well as power, performance, and area (PPA) analysis using OpenSTA. The design is mappd to standard cells from the target technology library, and timing, power, and area reports are extracted to evaluate the quality of the evolved approximate multiplier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program would save log files as well as the best evolved netlist file. In the log file you can get the equivelent transistor number, best fitness value and all the error metrics for each generation. The generated netlist file hasn't been mapped to a certain technology. Therefore, we need to map it to GF180 through: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvCEBjWwLwBi"
   },
   "outputs": [],
   "source": [
    "# Map the generated netlist to GF180\n",
    "!make map_netlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JHQNM15WW2K"
   },
   "source": [
    "Then we can extract the truth table of the generated approximate multiplier using the following command. In this step the mapped netlist would be instantialized in a testbench written in systemverilog and evaluated by behavior simulation using iVerilog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF3CYKLSLx-9"
   },
   "outputs": [],
   "source": [
    "# Extract the truth table of the generated approximate multiplier\n",
    "!make sim_approx_netlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo6WGc9_XAoK"
   },
   "source": [
    "We can also check the error metrics from the extracted truth table for verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckTzEyPJH1Sh"
   },
   "outputs": [],
   "source": [
    "# Todo: Error metrics verification\n",
    "!make check_approx_netlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Implementation of Deep Learning Tasks including Fashion-MNIST on LeNet-5 and CIFAR-100 on ResNet-18/ResNet-20 with Approximate Computing Unit\n",
    "\n",
    "To begin this section, we fine-tune an INT8 quantized ResNet-20 on the CIFAR-100 dataset. This run demonstrates how the approximate arithmetic units integrate with a full image-classification workload before extending to additional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization-aware training for CIFAR-100 ResNet-20 using helpers from train_qat_cifar100_resnet20.py\n",
    "import os, sys, json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "module_dir = Path(\"code/quant_code\")\n",
    "if not module_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Expected directory '{module_dir}' alongside this notebook.\")\n",
    "if str(module_dir.resolve()) not in sys.path:\n",
    "    sys.path.insert(0, str(module_dir.resolve()))\n",
    "repo_root = Path(\".\").resolve()\n",
    "\n",
    "from train_qat_cifar100_resnet20 import (\n",
    "    Cfg, get_loaders_cifar100, QResNet20CIFAR,\n",
    "    train_one_epoch, evaluate, set_activation_quant_enabled,\n",
    "    calibrate_activations, calibrate_weights\n",
    " )\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "cfg = Cfg()\n",
    "cfg.data_root = str((repo_root / \"datasets\").resolve())\n",
    "cfg.fp32_ckpt = str((repo_root / \"runs_fp32\" / \"resnet20_cifar100_fp32.pt\").resolve())\n",
    "cfg.out_dir = str((repo_root / \"runs_qat\" / \"resnet20\").resolve())\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "train_loader, test_loader = get_loaders_cifar100(cfg)\n",
    "model = QResNet20CIFAR(num_classes=cfg.num_classes, cfg=cfg).to(cfg.device)\n",
    "\n",
    "if cfg.fp32_ckpt and os.path.isfile(cfg.fp32_ckpt):\n",
    "    sd = torch.load(cfg.fp32_ckpt, map_location='cpu')\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    print(f'load_state: missing={len(missing)} unexpected={len(unexpected)}')\n",
    "else:\n",
    "    print('[Warn] fp32 checkpoint not found, training continues from scratch.')\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=cfg.lr_qat, momentum=cfg.momentum,\n",
    "    weight_decay=cfg.weight_decay, nesterov=cfg.nesterov\n",
    " )\n",
    "\n",
    "total_epochs = cfg.warmup_epochs + cfg.epochs_qat\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs, eta_min=cfg.eta_min)\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "\n",
    "set_activation_quant_enabled(model, in_enabled=False, out_enabled=False)\n",
    "print(f'[Warm-up] epochs={cfg.warmup_epochs}')\n",
    "for ep in range(1, cfg.warmup_epochs + 1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, cfg.device)\n",
    "    te_loss, te_acc = evaluate(model, test_loader, cfg.device)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'[W{ep:02d}] lr {cur_lr:.2e} | train {tr_loss:.4f}/{tr_acc*100:.2f}% | test {te_loss:.4f}/{te_acc*100:.2f}%')\n",
    "    scheduler.step()\n",
    "\n",
    "calibrate_weights(model)\n",
    "\n",
    "set_activation_quant_enabled(model, in_enabled=cfg.quantize_input, out_enabled=cfg.quantize_output)\n",
    "print(f'[Calib-1] collecting {cfg.calib_steps} mini-batches...')\n",
    "calibrate_activations(model, train_loader, cfg.device, cfg.calib_steps)\n",
    "\n",
    "total_qat = cfg.epochs_qat\n",
    "re_ep = int(total_qat * cfg.recalib_ratio) if cfg.recalib else -1\n",
    "\n",
    "for ep in range(1, total_qat + 1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, cfg.device)\n",
    "    te_loss, te_acc = evaluate(model, test_loader, cfg.device)\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"test_loss\"].append(te_loss);  history[\"test_acc\"].append(te_acc)\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'[QAT {ep:03d}] lr {cur_lr:.2e} | train {tr_loss:.4f}/{tr_acc*100:.2f}% | test {te_loss:.4f}/{te_acc*100:.2f}%')\n",
    "    scheduler.step()\n",
    "    if cfg.recalib and ep == re_ep:\n",
    "        print(f'[Calib-2 @ QAT {ep}] collecting {cfg.calib_steps} mini-batches...')\n",
    "        calibrate_activations(model, train_loader, cfg.device, cfg.calib_steps)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(cfg.out_dir, cfg.ckpt))\n",
    "with open(os.path.join(cfg.out_dir, cfg.hist), 'w') as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "te_loss, te_acc = evaluate(model, test_loader, cfg.device)\n",
    "print(f'INT8 final: loss {te_loss:.4f}, acc {te_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also run a diagnostic experiment: if we simply reuse the INT8 weights from the baseline network while swapping in the approximate multiplier, the classification accuracy collapses rapidly. This highlights that hardware-aware fine-tuning is essential—the quantized weights alone cannot compensate for the extra arithmetic error introduced by the approximate units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import time, torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from resnet20_lut import ResNet20LUTCfg, build_lut_resnet20, resolve_lut_table\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Cfg(ResNet20LUTCfg):\n",
    "    test_subset: int | None = None\n",
    "    lut_table_path: str | None = 'code/lut/truth_table_0.5.csv'\n",
    "    qat_ckpt: str = 'runs_qat/resnet20/resnet20_cifar100_qat_int8.pt'\n",
    "    # lut_table_path: str | None = None\n",
    "\n",
    "\n",
    "\n",
    "def get_test_loader(cfg: Cfg) -> DataLoader:\n",
    "    mean = [0.5071, 0.4865, 0.4409]\n",
    "    std = [0.2673, 0.2564, 0.2762]\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_set = datasets.CIFAR100(cfg.data_root, train=False, download=True, transform=test_tf)\n",
    "    if cfg.test_subset is not None and cfg.test_subset < len(test_set):\n",
    "        indices = torch.arange(cfg.test_subset)\n",
    "        test_set = Subset(test_set, indices)\n",
    "    return DataLoader(test_set, batch_size=cfg.batch_sz, shuffle=False,\n",
    "                      num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: str) -> tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if device.startswith('cuda'):\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    total_batches = len(loader)\n",
    "    for step, (x, y) in enumerate(loader, 1):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        if step % max(1, total_batches // 10) == 0 or step == total_batches:\n",
    "            print(f'[Eval] processed {step}/{total_batches} batches...')\n",
    "    if device.startswith('cuda'):\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.time() - t0\n",
    "    avg_loss = total_loss / max(total, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    return avg_loss, acc, elapsed\n",
    "\n",
    "cfg = Cfg()\n",
    "device = cfg.device\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if cfg.lut_table_path is not None:\n",
    "    base_dir = Path.cwd()\n",
    "    resolve_lut_table(cfg, base_dir)\n",
    "    print(f'[Info] Loaded LUT truth table: {cfg.lut_table_path}')\n",
    "\n",
    "print('[Info] Building LUT ResNet-20 and loading QAT checkpoint...')\n",
    "print(f'[Info] Using checkpoint at: {cfg.qat_ckpt}')\n",
    "model = build_lut_resnet20(cfg).to(device)\n",
    "\n",
    "print('[Info] Preparing CIFAR-100 test set...')\n",
    "test_loader = get_test_loader(cfg)\n",
    "\n",
    "print('[Info] Starting evaluation...')\n",
    "loss, acc, elapsed = evaluate(model, test_loader, device)\n",
    "samples = len(test_loader.dataset)\n",
    "print(f'[Result] test_loss={loss:.4f}, test_acc={acc*100:.2f}%, time={elapsed:.2f}s, throughput={samples/elapsed:.1f} samples/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now begin fine-tuning. Note that we substitute the gradient of the exact multiplier for the gradient of the approximate multiplier; this keeps the update direction aligned with reducing the network loss, so each step still moves us downhill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded LUT table: /home/junyi/projects/sscs-ose-code-a-chip.github.io/ISSCC26/CircuitsDNA/code/lut/truth_table_0.5.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "module_dir = Path(\"code/quant_code\")\n",
    "if not module_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Expected directory '{module_dir}' alongside this notebook.\")\n",
    "module_path = str(module_dir.resolve())\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from resnet20_lut import (\n",
    "    QResNet20CIFARLUT,\n",
    "    ResNet20LUTCfg,\n",
    "    build_lut_resnet20,\n",
    "    resolve_lut_table,\n",
    " )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Cfg(ResNet20LUTCfg):\n",
    "    qat_ckpt: str = 'runs_qat/resnet20/resnet20_cifar100_qat_int8.pt'  # QAT weight path\n",
    "    epochs: int = 5\n",
    "    lr: float = 5e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    momentum: float = 0.9\n",
    "    nesterov: bool = True\n",
    "    log_interval: int = 100\n",
    "    eta_min: float = 1e-5\n",
    "    output_dir: str = 'runs_qat/resnet20'\n",
    "    save_path: str | None = None\n",
    "    resume_path: str | None = None\n",
    "    history_path: str | None = None\n",
    "    lut_table_path: str | None = 'code/lut/truth_table_0.5.csv'\n",
    "    num_workers: int = 0  # avoid duplicate prints from worker processes\n",
    "\n",
    "\n",
    "def get_loaders(cfg: Cfg) -> tuple[DataLoader, DataLoader]:\n",
    "    mean = [0.5071, 0.4865, 0.4409]\n",
    "    std = [0.2673, 0.2564, 0.2762]\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    train_set = datasets.CIFAR100(cfg.data_root, train=True, download=True, transform=train_tf)\n",
    "    test_set = datasets.CIFAR100(cfg.data_root, train=False, download=True, transform=test_tf)\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=cfg.batch_sz, shuffle=True, num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=cfg.batch_sz, shuffle=False, num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def infer_lut_tag(lut_path: str | None) -> str:\n",
    "    if not lut_path:\n",
    "        return 'exact'\n",
    "    stem = os.path.splitext(os.path.basename(lut_path))[0]\n",
    "    candidate = stem.split('_')[-1] if '_' in stem else stem\n",
    "    candidate = candidate.strip()\n",
    "    return candidate or 'custom'\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: QResNet20CIFARLUT,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    log_interval: int,\n",
    " ) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    for step, (x, y) in enumerate(loader, 1):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        if step % max(1, log_interval) == 0:\n",
    "            print(\n",
    "                f'[Train] step={step:04d} loss={loss.item():.4f} acc={(correct/total)*100:.2f}%',\n",
    "                flush=True,\n",
    "            )\n",
    "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: QResNet20CIFARLUT, loader: DataLoader, device: str) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "\n",
    "def build_model(cfg: Cfg) -> QResNet20CIFARLUT:\n",
    "    if cfg.qat_ckpt and os.path.isfile(cfg.qat_ckpt):\n",
    "        model = build_lut_resnet20(cfg)\n",
    "    elif cfg.resume_path:\n",
    "        print('[Warn] Initial QAT checkpoint not found, falling back to resume only.')\n",
    "        model = QResNet20CIFARLUT(num_classes=cfg.num_classes, cfg=cfg)\n",
    "    else:\n",
    "        raise FileNotFoundError('Missing initial QAT weights; set cfg.qat_ckpt or provide resume_path')\n",
    "    if cfg.resume_path and os.path.isfile(cfg.resume_path):\n",
    "        print(f'[Info] Resume from {cfg.resume_path}')\n",
    "        state = torch.load(cfg.resume_path, map_location='cpu')\n",
    "        model.load_state_dict(state, strict=False)\n",
    "    return model\n",
    "\n",
    "cfg = Cfg()\n",
    "device = cfg.device\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if cfg.lut_table_path is not None:\n",
    "    base_dir = Path.cwd()\n",
    "    resolve_lut_table(cfg, base_dir)\n",
    "    print(f'[Info] Loaded LUT table: {cfg.lut_table_path}')\n",
    "lut_tag = infer_lut_tag(cfg.lut_table_path)\n",
    "if cfg.save_path is None:\n",
    "    cfg.save_path = os.path.join(cfg.output_dir, f'resnet20_cifar100_qat_int8_lut_{lut_tag}_ft.pt')\n",
    "if cfg.history_path is None:\n",
    "    cfg.history_path = os.path.join(cfg.output_dir, f'history_ft_resnet20_lut_{lut_tag}.json')\n",
    "train_loader, test_loader = get_loaders(cfg)\n",
    "model = build_model(cfg).to(device)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay, nesterov=cfg.nesterov\n",
    " )\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs, eta_min=cfg.eta_min)\n",
    "best_acc = 0.0\n",
    "history: dict[str, list[float]] = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "}\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, cfg.log_interval)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - t0\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}] {elapsed:.1f}s | train {train_loss:.4f}/{train_acc*100:.2f}% | \"\n",
    "        f\"test {test_loss:.4f}/{test_acc*100:.2f}% | lr={optimizer.param_groups[0]['lr']:.2e}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        os.makedirs(os.path.dirname(cfg.save_path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), cfg.save_path)\n",
    "        print(f'[Info] Saved new best model -> {cfg.save_path} (acc={best_acc*100:.2f}%)', flush=True)\n",
    "print(f'[Done] Best test accuracy: {best_acc*100:.2f}%')\n",
    "os.makedirs(os.path.dirname(cfg.history_path), exist_ok=True)\n",
    "with open(cfg.history_path, 'w') as f:\n",
    "    json.dump(history, f)\n",
    "print(f'[Info] Training history saved to {cfg.history_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2VD4XAwtLYa"
   },
   "source": [
    "---\n",
    "## 7. Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alRNBD1WWbd4"
   },
   "source": [
    "---\n",
    "## 8. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ngSu6jzXBrD"
   },
   "source": [
    "In conclusion, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
